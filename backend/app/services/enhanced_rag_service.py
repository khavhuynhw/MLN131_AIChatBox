import google.generativeai as genai
from .vector_store import SimpleVectorStore
# from .web_data_collector import WebDataCollector  # Disabled to prevent external API calls
import os
from dotenv import load_dotenv
import json
from datetime import datetime
from typing import List
from urllib.parse import quote
import unicodedata
import re

load_dotenv()

class EnhancedRAGService:
    def __init__(self):
        self.vector_store = SimpleVectorStore()
        # self.data_collector = WebDataCollector()  # Disabled to prevent external API calls
        
        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
        self.model = genai.GenerativeModel('gemini-2.5-flash')
        
        self.last_update = None
        print("Enhanced RAG Service v2.1 ready with improved citations!")
    
    def add_chapter03_corpus(self):
        """ThÃªm corpus ChÆ°Æ¡ng 03: Chá»§ nghÄ©a xÃ£ há»™i vÃ  thá»i ká»³ quÃ¡ Ä‘á»™ lÃªn chá»§ nghÄ©a xÃ£ há»™i"""
        # Load Chapter 03 content from file
        chapter03_path = os.path.join(os.path.dirname(__file__), "../../data/book/chuong3.md")
        
        if not os.path.exists(chapter03_path):
            print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y file {chapter03_path}")
            return
        
        with open(chapter03_path, 'r', encoding='utf-8') as f:
            chapter03_content = f.read()
        
        # Split content into meaningful chunks
        comprehensive_docs = self._split_chapter03_content(chapter03_content)
        
        # Create metadata for Chapter 03 content
        comprehensive_metadata = []
        for i, doc in enumerate(comprehensive_docs):
            metadata = {
                "source": "GiÃ¡o trÃ¬nh Chá»§ nghÄ©a xÃ£ há»™i khoa há»c (K-2021)",
                "document": "ChÆ°Æ¡ng III: Chá»§ nghÄ©a xÃ£ há»™i vÃ  thá»i ká»³ quÃ¡ Ä‘á»™ lÃªn chá»§ nghÄ©a xÃ£ há»™i",
                "topic": "chá»§ nghÄ©a xÃ£ há»™i",
                "page": f"chunk_{i+1}",
                "credibility_score": 100,
                "source_type": "textbook"
            }
            comprehensive_metadata.append(metadata)
        
        self.vector_store.add_documents(comprehensive_docs, comprehensive_metadata)
        print(f"Added {len(comprehensive_docs)} documents from Chapter 03 with detailed citations")
    
    def _split_chapter03_content(self, content: str) -> List[str]:
        """Chia ná»™i dung ChÆ°Æ¡ng 03 thÃ nh cÃ¡c Ä‘oáº¡n cÃ³ Ã½ nghÄ©a"""
        # Split by major sections
        sections = re.split(r'\n(?=[IVX]+\.)', content)
        
        chunks = []
        for section in sections:
            if not section.strip():
                continue
                
            # Further split by paragraphs
            paragraphs = section.split('\n\n')
            current_chunk = ""
            
            for paragraph in paragraphs:
                paragraph = paragraph.strip()
                if not paragraph:
                    continue
                    
                # If adding this paragraph would make chunk too long, save current chunk
                if len(current_chunk) + len(paragraph) > 1000 and current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = paragraph
                else:
                    if current_chunk:
                        current_chunk += "\n\n" + paragraph
                    else:
                        current_chunk = paragraph
            
            # Add remaining chunk
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
        
        return chunks

    def load_chapter03_data(self):
        """Load dá»¯ liá»‡u ChÆ°Æ¡ng 03 vÃ o vector store"""
        print("ğŸ”„ Äang táº£i dá»¯ liá»‡u ChÆ°Æ¡ng 03...")
        self.add_chapter03_corpus()
        print("âœ… HoÃ n thÃ nh táº£i dá»¯ liá»‡u ChÆ°Æ¡ng 03!")

    def ingest_markdown_folder(self, folder_path: str):
        """Äá»c táº¥t cáº£ cÃ¡c file .md trong thÆ° má»¥c vÃ  Ä‘Æ°a vÃ o vector store.
        - Má»i citation sáº½ trá» vá» 'GiÃ¡o trÃ¬nh Chá»§ nghÄ©a xÃ£ há»™i khoa há»c'.
        - 'document' lÃ  tÃªn file (khÃ´ng Ä‘uÃ´i), vÃ­ dá»¥: 'chuong3' -> 'ChÆ°Æ¡ng 03'.
        """
        try:
            if not os.path.exists(folder_path):
                os.makedirs(folder_path, exist_ok=True)
                print(f"Táº¡o thÆ° má»¥c book: {folder_path}")

            md_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.md')]
            if not md_files:
                print(f"KhÃ´ng tÃ¬m tháº¥y file .md trong {folder_path}")
                return

            all_docs, all_metas = [], []
            for fname in md_files:
                fpath = os.path.join(folder_path, fname)
                try:
                    with open(fpath, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                    if not content:
                        continue

                    # Cáº¯t nhá» ná»™i dung Ä‘á»ƒ index
                    chunks = self.split_text(content, max_length=700)
                    # TÃªn hiá»ƒn thá»‹ cá»§a tÃ i liá»‡u
                    base = os.path.splitext(fname)[0]
                    display_name = base.replace('-', ' ').title()
                    # Chuáº©n hÃ³a tÃªn hiá»ƒn thá»‹ vá»›i dáº¥u tiáº¿ng Viá»‡t cho cÃ¡c trang chÃ­nh
                    bl = base.lower()
                    if bl == 'tu-tuong-ho-chi-minh':
                        display_name = 'Chá»§ nghÄ©a xÃ£ há»™i vÃ  thá»i ká»³ quÃ¡ Ä‘á»™'
                    elif bl == 'muc-luc':
                        display_name = 'Má»¥c lá»¥c'
                    elif bl in ('chuong1', 'chuong-1'):
                        display_name = 'ChÆ°Æ¡ng I'
                    elif bl in ('chuong2', 'chuong-2'):
                        display_name = 'ChÆ°Æ¡ng II'
                    elif bl in ('chuong3', 'chuong-3'):
                        display_name = 'ChÆ°Æ¡ng III'
                    elif bl in ('chuong4', 'chuong-4'):
                        display_name = 'ChÆ°Æ¡ng IV'
                    elif bl in ('chuong5', 'chuong-5'):
                        display_name = 'ChÆ°Æ¡ng V'
                    elif bl in ('chuong6', 'chuong-6'):
                        display_name = 'ChÆ°Æ¡ng VI'

                    for ch in chunks:
                        all_docs.append(ch)
                        all_metas.append({
                            "source": "GiÃ¡o trÃ¬nh Chá»§ nghÄ©a xÃ£ há»™i khoa há»c (K-2021)",
                            "document": display_name,
                            "page": base,
                            "credibility_score": 95,
                            "source_type": "document",
                            "url": f"/book/{base}"
                        })
                except Exception as e:
                    print(f"Error reading {fname}: {e}")

            if all_docs:
                self.vector_store.add_documents(all_docs, all_metas)
                print(f"Ingested {len(all_docs)} segments from markdown folder {folder_path}")
        except Exception as e:
            print(f"Error ingesting markdown: {e}")
    
    def update_knowledge_base(self, force_update=False):
        """Cáº­p nháº­t knowledge base chá»‰ tá»« ChÆ°Æ¡ng 03: Chá»§ nghÄ©a xÃ£ há»™i vÃ  thá»i ká»³ quÃ¡ Ä‘á»™ lÃªn chá»§ nghÄ©a xÃ£ há»™i.
        Náº¿u force_update=True: xÃ³a index cÅ© trÆ°á»›c khi ingest Ä‘á»ƒ trÃ¡nh láº«n nguá»“n cÅ©.
        """
        if force_update:
            self.vector_store.reset()
        self.load_chapter03_data()
        self.last_update = datetime.now()
        print("Knowledge base updated tá»« ChÆ°Æ¡ng 03: Chá»§ nghÄ©a xÃ£ há»™i vÃ  thá»i ká»³ quÃ¡ Ä‘á»™ lÃªn chá»§ nghÄ©a xÃ£ há»™i")
    
    def split_text(self, text: str, max_length: int = 700) -> List[str]:
        """Chia nhá» theo Ä‘oáº¡n (paragraph-first) Ä‘á»ƒ giá»¯ nguyÃªn cÃ¡c khá»‘i Ä‘á»‹nh nghÄ©a/trÃ­ch dáº«n.
        - Æ¯u tiÃªn tÃ¡ch theo 2+ dÃ²ng tráº¯ng.
        - Náº¿u Ä‘oáº¡n quÃ¡ dÃ i, fallback tÃ¡ch theo cÃ¢u '. '.
        """
        paras = [p.strip() for p in re.split(r"\n\s*\n+", text) if p.strip()]
        chunks: List[str] = []
        for p in paras:
            if len(p) <= max_length:
                chunks.append(p)
            else:
                sentences = p.split('. ')
                current = ""
                for s in sentences:
                    if len(current) + len(s) + 2 <= max_length:
                        current += (s + ". ")
                    else:
                        if current:
                            chunks.append(current.strip())
                        current = s + ". "
                if current:
                    chunks.append(current.strip())
        return chunks
    
    def _normalize(self, s: str) -> str:
        """Chuáº©n hÃ³a text: loáº¡i bá» dáº¥u tiáº¿ng Viá»‡t vÃ  chuyá»ƒn thÃ nh chá»¯ thÆ°á»ng"""
        if not s:
            return ''
        
        # Báº£ng chuyá»ƒn Ä‘á»•i kÃ½ tá»± cÃ³ dáº¥u tiáº¿ng Viá»‡t
        vietnamese_chars = {
            'Ã ': 'a', 'Ã¡': 'a', 'áº£': 'a', 'Ã£': 'a', 'áº¡': 'a',
            'Äƒ': 'a', 'áº±': 'a', 'áº¯': 'a', 'áº³': 'a', 'áºµ': 'a', 'áº·': 'a',
            'Ã¢': 'a', 'áº§': 'a', 'áº¥': 'a', 'áº©': 'a', 'áº«': 'a', 'áº­': 'a',
            'Ã¨': 'e', 'Ã©': 'e', 'áº»': 'e', 'áº½': 'e', 'áº¹': 'e',
            'Ãª': 'e', 'á»': 'e', 'áº¿': 'e', 'á»ƒ': 'e', 'á»…': 'e', 'á»‡': 'e',
            'Ã¬': 'i', 'Ã­': 'i', 'á»‰': 'i', 'Ä©': 'i', 'á»‹': 'i',
            'Ã²': 'o', 'Ã³': 'o', 'á»': 'o', 'Ãµ': 'o', 'á»': 'o',
            'Ã´': 'o', 'á»“': 'o', 'á»‘': 'o', 'á»•': 'o', 'á»—': 'o', 'á»™': 'o',
            'Æ¡': 'o', 'á»': 'o', 'á»›': 'o', 'á»Ÿ': 'o', 'á»¡': 'o', 'á»£': 'o',
            'Ã¹': 'u', 'Ãº': 'u', 'á»§': 'u', 'Å©': 'u', 'á»¥': 'u',
            'Æ°': 'u', 'á»«': 'u', 'á»©': 'u', 'á»­': 'u', 'á»¯': 'u', 'á»±': 'u',
            'á»³': 'y', 'Ã½': 'y', 'á»·': 'y', 'á»¹': 'y', 'á»µ': 'y',
            'Ä‘': 'd', 'Ä': 'd'
        }
        
        # Chuyá»ƒn thÃ nh chá»¯ thÆ°á»ng
        s = s.lower()
        
        # Thay tháº¿ cÃ¡c kÃ½ tá»± cÃ³ dáº¥u
        for vn_char, latin_char in vietnamese_chars.items():
            s = s.replace(vn_char, latin_char)
        
        # Loáº¡i bá» cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t, chá»‰ giá»¯ chá»¯ vÃ  sá»‘
        s = re.sub(r'[^a-z0-9\s]', '', s)
        
        # Chuáº©n hÃ³a khoáº£ng tráº¯ng
        s = re.sub(r'\s+', ' ', s).strip()
        
        return s

    def _slug_to_title(self, slug: str) -> str:
        s = (slug or '').lower().strip()
        mapping = {
            'chuong1': 'ChÆ°Æ¡ng I',
            'chuong2': 'ChÆ°Æ¡ng II',
            'chuong3': 'ChÆ°Æ¡ng III',
            'chuong4': 'ChÆ°Æ¡ng IV',
            'chuong5': 'ChÆ°Æ¡ng V',
            'chuong6': 'ChÆ°Æ¡ng VI',
        }
        return mapping.get(s, slug or '')
    
    def detect_chapter_summary_request(self, question: str) -> tuple[bool, str]:
        """PhÃ¡t hiá»‡n yÃªu cáº§u tÃ³m táº¯t chÆ°Æ¡ng vÃ  tráº£ vá» (is_summary, chapter_name)"""
        q_norm = self._normalize(question)
        summary_keywords = ['tom tat', 'tom tac', 'tong ket', 'noi dung chinh', 'yeu to']
        chapter_keywords = ['chuong', 'phan']
        
        # Kiá»ƒm tra cÃ³ tá»« khÃ³a tÃ³m táº¯t
        has_summary = any(kw in q_norm for kw in summary_keywords)
        has_chapter = any(kw in q_norm for kw in chapter_keywords)
        
        if not (has_summary and has_chapter):
            return False, ""
        
        # TÃ¬m sá»‘ chÆ°Æ¡ng
        import re
        # TÃ¬m chÆ°Æ¡ng báº±ng sá»‘ La MÃ£ hoáº·c sá»‘ Arabic
        chapter_match = re.search(r'chÆ°Æ¡ng\s*(\d+|[ivxlcdm]+)', q_norm)
        if chapter_match:
            chapter_num = chapter_match.group(1)
            # Chuyá»ƒn sá»‘ La MÃ£ thÃ nh sá»‘ Arabic náº¿u cáº§n
            roman_to_num = {'i': '1', 'ii': '2', 'iii': '3', 'iv': '4', 'v': '5', 'vi': '6'}
            if chapter_num.lower() in roman_to_num:
                chapter_num = roman_to_num[chapter_num.lower()]
            return True, f"chuong{chapter_num}"
        
        # TÃ¬m theo pattern "chÆ°Æ¡ng X"
        for i in range(1, 7):
            if f"chuong {i}" in q_norm or f"chuong{i}" in q_norm:
                return True, f"chuong{i}"
        
        return False, ""
    
    def detect_mindmap_request(self, question: str) -> bool:
        """PhÃ¡t hiá»‡n yÃªu cáº§u táº¡o sÆ¡ Ä‘á»“ tÆ° duy"""
        q_norm = self._normalize(question)
        mindmap_keywords = [
            'tao so do tu duy',
            'so do tu duy',
            've so do',
            'so do ve',
            'bieu do',
            'so do',
            'mindmap',
            'mind map',
            'mermaid mindmap',
            'hien thi so do',
            'tao so do'
        ]
        
        is_mindmap = any(kw in q_norm for kw in mindmap_keywords)
        print(f"ğŸ” MINDMAP DEBUG: '{question}' -> normalized: '{q_norm}' -> is_mindmap: {is_mindmap}")
        return is_mindmap
    
    def detect_off_topic_question(self, question: str) -> bool:
        """PhÃ¡t hiá»‡n cÃ¢u há»i khÃ´ng liÃªn quan Ä‘áº¿n chá»§ nghÄ©a xÃ£ há»™i"""
        q_norm = self._normalize(question)
        
        # CÃ¡c tá»« khÃ³a liÃªn quan Ä‘áº¿n chá»§ nghÄ©a xÃ£ há»™i
        socialism_keywords = [
            'chu nghia xa hoi', 'chá»§ nghÄ©a xÃ£ há»™i', 'cnxh', 'cnx',
            'thoi ky qua do', 'thá»i ká»³ quÃ¡ Ä‘á»™', 'qua do', 'quÃ¡ Ä‘á»™',
            'mac lenin', 'mÃ¡c lÃªnin', 'mac', 'mÃ¡c', 'lenin', 'lÃªnin',
            'cong san', 'cá»™ng sáº£n', 'tu ban', 'tÆ° báº£n', 'giai cap', 'giai cáº¥p',
            'cach mang', 'cÃ¡ch máº¡ng', 'vo san', 'vÃ´ sáº£n', 'tu san', 'tÆ° sáº£n',
            'dang cong san', 'Ä‘áº£ng cá»™ng sáº£n', 'nha nuoc', 'nhÃ  nÆ°á»›c',
            'kinh te', 'kinh táº¿', 'san xuat', 'sáº£n xuáº¥t', 'quan he', 'quan há»‡',
            'hinh thai', 'hÃ¬nh thÃ¡i', 'xÃ£ há»™i', 'xa hoi', 'che do', 'cháº¿ Ä‘á»™',
            'dac trung', 'Ä‘áº·c trÆ°ng', 'ban chat', 'báº£n cháº¥t', 'muc tieu', 'má»¥c tiÃªu',
            'phuong huong', 'phÆ°Æ¡ng hÆ°á»›ng', 'xay dung', 'xÃ¢y dá»±ng', 'phat trien', 'phÃ¡t triá»ƒn'
        ]
        
        # CÃ¡c tá»« khÃ³a khÃ´ng liÃªn quan
        off_topic_keywords = [
            'thoi tiet', 'thá»i tiáº¿t', 'weather', 'mua', 'mÆ°a', 'nang', 'náº¯ng',
            'am nhac', 'Ã¢m nhac', 'music', 'nhac', 'nháº¡c', 'bai hat', 'bÃ i hÃ¡t',
            'phim', 'movie', 'film', 'dien anh', 'Ä‘iá»‡n áº£nh', 'tv', 'tivi',
            'the thao', 'thá»ƒ thao', 'sport', 'bong da', 'bÃ³ng Ä‘Ã¡', 'football',
            'game', 'tro choi', 'trÃ² chÆ¡i', 'video game', 'game online',
            'du lich', 'du lá»‹ch', 'travel', 'di choi', 'Ä‘i chÆ¡i', 'nghi mat', 'nghá»‰ mÃ¡t',
            'an uong', 'Äƒn uá»‘ng', 'food', 'thuc an', 'thá»©c Äƒn', 'mon an', 'mÃ³n Äƒn',
            'nau an', 'náº¥u Äƒn', 'cach nau', 'cÃ¡ch náº¥u', 'pho', 'phá»Ÿ', 'bun', 'bÃºn',
            'thoi trang', 'thá»i trang', 'fashion', 'quan ao', 'quáº§n Ã¡o', 'giay dep', 'giÃ y dÃ©p',
            'lam dep', 'lÃ m Ä‘áº¹p', 'beauty', 'my pham', 'má»¹ pháº©m', 'trang diem', 'trang Ä‘iá»ƒm',
            'cong nghe', 'cÃ´ng nghá»‡', 'technology', 'dien thoai', 'Ä‘iá»‡n thoáº¡i', 'smartphone',
            'may tinh', 'mÃ¡y tÃ­nh', 'computer', 'laptop', 'internet', 'wifi',
            'hoc tap', 'há»c táº­p', 'study', 'hoc', 'há»c', 'bai tap', 'bÃ i táº­p',
            'cong viec', 'cÃ´ng viá»‡c', 'job', 'viec lam', 'viá»‡c lÃ m', 'tuyen dung', 'tuyá»ƒn dá»¥ng',
            'tinh yeu', 'tÃ¬nh yÃªu', 'love', 'yeu', 'yÃªu', 'hen ho', 'háº¹n hÃ²',
            'gia dinh', 'gia Ä‘Ã¬nh', 'family', 'bo me', 'bá»‘ máº¹', 'cha me', 'cha máº¹',
            'ban be', 'báº¡n bÃ¨', 'friend', 'ban', 'báº¡n', 'tinh ban', 'tÃ¬nh báº¡n',
            'mua sam', 'mua sáº¯m', 'shopping', 'mua', 'mua', 'ban', 'bÃ¡n', 'gia', 'giÃ¡'
        ]
        
        # CÃ¡c tá»« khÃ³a cáº£m tÃ­nh/Ä‘Ã¡nh giÃ¡ chá»§ quan
        emotional_keywords = [
            'tot hay khong', 'tá»‘t hay khÃ´ng', 'hay hay khong', 'hay hay khÃ´ng',
            'co tot khong', 'cÃ³ tá»‘t khÃ´ng', 'tot nhat', 'tá»‘t nháº¥t', 'hay nhat', 'hay nháº¥t',
            'danh gia', 'Ä‘Ã¡nh giÃ¡', 'y kien', 'Ã½ kiáº¿n', 'suy nghi', 'suy nghÄ©',
            'cam nhan', 'cáº£m nháº­n', 'cam giac', 'cáº£m giÃ¡c', 'thich', 'thÃ­ch',
            'khong thich', 'khÃ´ng thÃ­ch', 'ghet', 'ghÃ©t', 'yeu', 'yÃªu',
            'thuong', 'thÆ°Æ¡ng', 'ghe', 'ghÃª', 'kinh', 'kinh khá»§ng',
            'tuyet voi', 'tuyá»‡t vá»i', 'kinh khung', 'kinh khá»§ng', 'toi te', 'tá»“i tá»‡',
            'xau', 'xáº¥u', 'dep', 'Ä‘áº¹p', 'xinh', 'xinh Ä‘áº¹p', 'dep trai', 'Ä‘áº¹p trai',
            'thong minh', 'thÃ´ng minh', 'ngu', 'ngu ngá»‘c', 'stupid', 'smart',
            'good', 'bad', 'excellent', 'terrible', 'awesome', 'horrible'
        ]
        
        # Kiá»ƒm tra xem cÃ³ tá»« khÃ³a liÃªn quan Ä‘áº¿n chá»§ nghÄ©a xÃ£ há»™i khÃ´ng
        has_socialism_keywords = any(keyword in q_norm for keyword in socialism_keywords)
        
        # Kiá»ƒm tra xem cÃ³ tá»« khÃ³a khÃ´ng liÃªn quan khÃ´ng
        has_off_topic_keywords = any(keyword in q_norm for keyword in off_topic_keywords)
        
        # Kiá»ƒm tra xem cÃ³ tá»« khÃ³a cáº£m tÃ­nh/Ä‘Ã¡nh giÃ¡ chá»§ quan khÃ´ng
        has_emotional_keywords = any(keyword in q_norm for keyword in emotional_keywords)
        
        # Náº¿u cÃ³ tá»« khÃ³a khÃ´ng liÃªn quan vÃ  khÃ´ng cÃ³ tá»« khÃ³a liÃªn quan Ä‘áº¿n CNXH
        is_off_topic = has_off_topic_keywords and not has_socialism_keywords
        
        # Náº¿u cÃ³ tá»« khÃ³a cáº£m tÃ­nh (báº¥t ká»ƒ cÃ³ tá»« khÃ³a CNXH hay khÃ´ng)
        is_emotional = has_emotional_keywords
        
        # Káº¿t há»£p cáº£ hai Ä‘iá»u kiá»‡n
        is_inappropriate = is_off_topic or is_emotional
        
        print(f"ğŸ” OFF-TOPIC DEBUG: '{question}' -> normalized: '{q_norm}' -> is_off_topic: {is_off_topic}, is_emotional: {is_emotional}, is_inappropriate: {is_inappropriate}")
        return is_inappropriate
    
    def _handle_off_topic_question(self, question: str):
        """Xá»­ lÃ½ cÃ¢u há»i khÃ´ng liÃªn quan hoáº·c cáº£m tÃ­nh vá» chá»§ nghÄ©a xÃ£ há»™i"""
        print(f"ğŸš« Handling off-topic/emotional question: {question}")
        
        # Kiá»ƒm tra xem cÃ³ pháº£i cÃ¢u há»i cáº£m tÃ­nh khÃ´ng
        q_norm = self._normalize(question)
        emotional_keywords = [
            'tot hay khong', 'tá»‘t hay khÃ´ng', 'hay hay khong', 'hay hay khÃ´ng',
            'co tot khong', 'cÃ³ tá»‘t khÃ´ng', 'danh gia', 'Ä‘Ã¡nh giÃ¡', 'y kien', 'Ã½ kiáº¿n'
        ]
        is_emotional = any(keyword in q_norm for keyword in emotional_keywords)
        
        if is_emotional:
            response = f"""TÃ´i hiá»ƒu báº¡n muá»‘n Ä‘Ã¡nh giÃ¡ vá» chá»§ nghÄ©a xÃ£ há»™i, nhÆ°ng tÃ´i lÃ  chatbot há»c thuáº­t chuyÃªn cung cáº¥p **thÃ´ng tin khÃ¡ch quan** vá» **Chá»§ nghÄ©a xÃ£ há»™i vÃ  thá»i ká»³ quÃ¡ Ä‘á»™ lÃªn chá»§ nghÄ©a xÃ£ há»™i**.

### ğŸ¯ Thay vÃ¬ Ä‘Ã¡nh giÃ¡ chá»§ quan, tÃ´i cÃ³ thá»ƒ giÃºp báº¡n hiá»ƒu:

**ğŸ“– Vá» máº·t lÃ½ luáº­n:**
- Äá»‹nh nghÄ©a chá»§ nghÄ©a xÃ£ há»™i theo 4 gÃ³c Ä‘á»™
- Äáº·c trÆ°ng báº£n cháº¥t cá»§a chá»§ nghÄ©a xÃ£ há»™i
- Quan Ä‘iá»ƒm cá»§a MÃ¡c - LÃªnin vá» CNXH

**ğŸ—ï¸ Vá» máº·t thá»±c tiá»…n:**
- Thá»i ká»³ quÃ¡ Ä‘á»™ lÃªn chá»§ nghÄ©a xÃ£ há»™i
- Sá»± váº­n dá»¥ng cá»§a Äáº£ng Cá»™ng sáº£n Viá»‡t Nam
- Má»¥c tiÃªu vÃ  phÆ°Æ¡ng hÆ°á»›ng xÃ¢y dá»±ng CNXH

### ğŸ’¡ CÃ¢u há»i há»c thuáº­t phÃ¹ há»£p:
- "Chá»§ nghÄ©a xÃ£ há»™i lÃ  gÃ¬?"
- "Äáº·c trÆ°ng cá»§a chá»§ nghÄ©a xÃ£ há»™i?"
- "Thá»i ká»³ quÃ¡ Ä‘á»™ cÃ³ Ä‘áº·c Ä‘iá»ƒm gÃ¬?"
- "LÃªnin nháº¥n máº¡nh Ä‘iá»u gÃ¬?"

HÃ£y há»i tÃ´i vá» nhá»¯ng khÃ­a cáº¡nh há»c thuáº­t nÃ y Ä‘á»ƒ cÃ³ cÃ¡i nhÃ¬n toÃ n diá»‡n! ğŸ“š"""
        else:
            response = f"""Xin lá»—i, tÃ´i lÃ  chatbot chuyÃªn vá» **Chá»§ nghÄ©a xÃ£ há»™i vÃ  thá»i ká»³ quÃ¡ Ä‘á»™ lÃªn chá»§ nghÄ©a xÃ£ há»™i**. 

TÃ´i khÃ´ng thá»ƒ tráº£ lá»i cÃ¢u há»i vá» chá»§ Ä‘á» khÃ¡c, nhÆ°ng tÃ´i cÃ³ thá»ƒ giÃºp báº¡n tÃ¬m hiá»ƒu vá»:

### ğŸ“š CÃ¡c chá»§ Ä‘á» tÃ´i cÃ³ thá»ƒ há»— trá»£:
- **Äá»‹nh nghÄ©a chá»§ nghÄ©a xÃ£ há»™i** (4 gÃ³c Ä‘á»™ tiáº¿p cáº­n)
- **Äáº·c trÆ°ng báº£n cháº¥t** cá»§a chá»§ nghÄ©a xÃ£ há»™i
- **Thá»i ká»³ quÃ¡ Ä‘á»™** lÃªn chá»§ nghÄ©a xÃ£ há»™i
- **Quan Ä‘iá»ƒm cá»§a MÃ¡c - LÃªnin** vá» chá»§ nghÄ©a xÃ£ há»™i
- **Sá»± váº­n dá»¥ng** cá»§a Äáº£ng Cá»™ng sáº£n Viá»‡t Nam
- **Má»¥c tiÃªu vÃ  phÆ°Æ¡ng hÆ°á»›ng** xÃ¢y dá»±ng CNXH á»Ÿ Viá»‡t Nam

### ğŸ’¡ Gá»£i Ã½ cÃ¢u há»i:
- "Chá»§ nghÄ©a xÃ£ há»™i lÃ  gÃ¬?"
- "Äáº·c trÆ°ng cá»§a chá»§ nghÄ©a xÃ£ há»™i?"
- "Thá»i ká»³ quÃ¡ Ä‘á»™ cÃ³ Ä‘áº·c Ä‘iá»ƒm gÃ¬?"
- "LÃªnin nháº¥n máº¡nh Ä‘iá»u gÃ¬?"

HÃ£y thá»­ há»i tÃ´i vá» nhá»¯ng chá»§ Ä‘á» trÃªn nhÃ©! ğŸ˜Š"""
        
        return {
            "answer": response,
            "sources": ["HÆ°á»›ng dáº«n sá»­ dá»¥ng chatbot"],
            "confidence": 100
        }
    
    def get_full_chapter_content(self, chapter_name: str) -> str:
        """Äá»c toÃ n bá»™ ná»™i dung cá»§a má»™t chÆ°Æ¡ng tá»« file .md"""
        book_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "data", "book"))
        chapter_file = os.path.join(book_dir, f"{chapter_name}.md")
        
        try:
            if os.path.exists(chapter_file):
                with open(chapter_file, 'r', encoding='utf-8') as f:
                    return f.read().strip()
            else:
                print(f"File not found: {chapter_file}")
                return ""
        except Exception as e:
            print(f"Error reading file {chapter_file}: {e}")
            return ""

    def generate_response_with_sources(self, question: str):
        """Generate response vá»›i improved citations.
        - Náº¿u tÃ¬m tháº¥y ná»™i dung trong .md: chá»‰ Ä‘Æ°á»£c phÃ©p tráº£ lá»i dá»±a trÃªn cÃ¡c Ä‘oáº¡n trÃ­ch (khÃ´ng thÃªm kiáº¿n thá»©c ngoÃ i).
        - Náº¿u khÃ´ng tÃ¬m tháº¥y: fallback sang Gemini tráº£ lá»i chung (ghi rÃµ lÃ  khÃ´ng cÃ³ trÃ­ch dáº«n .md).
        - Xá»­ lÃ½ Ä‘áº·c biá»‡t cho yÃªu cáº§u tÃ³m táº¯t chÆ°Æ¡ng: Ä‘á»c toÃ n bá»™ ná»™i dung chÆ°Æ¡ng.
        """
        try:
            print(f"ğŸ¯ RAG SERVICE: Processing question: '{question}'")
            
            # Kiá»ƒm tra xem cÃ³ pháº£i yÃªu cáº§u tÃ³m táº¯t chÆ°Æ¡ng khÃ´ng
            is_chapter_summary, chapter_name = self.detect_chapter_summary_request(question)
            
            if is_chapter_summary and chapter_name:
                print(f"ğŸ“– CHAPTER SUMMARY detected: {chapter_name}")
                # Xá»­ lÃ½ Ä‘áº·c biá»‡t cho tÃ³m táº¯t chÆ°Æ¡ng
                return self._handle_chapter_summary(question, chapter_name)
            
            # Kiá»ƒm tra xem cÃ³ pháº£i yÃªu cáº§u táº¡o sÆ¡ Ä‘á»“ tÆ° duy khÃ´ng
            if self.detect_mindmap_request(question):
                print(f"ğŸ§  MINDMAP REQUEST detected!")
                return self._handle_mindmap_request(question)
            
            # Kiá»ƒm tra xem cÃ³ pháº£i cÃ¢u há»i khÃ´ng liÃªn quan Ä‘áº¿n chá»§ nghÄ©a xÃ£ há»™i khÃ´ng
            if self.detect_off_topic_question(question):
                print(f"ğŸš« OFF-TOPIC QUESTION detected!")
                return self._handle_off_topic_question(question)
            
            # TÄƒng sá»‘ lÆ°á»£ng káº¿t quáº£ vÃ  Æ°u tiÃªn Ä‘oáº¡n chá»©a Ä‘á»‹nh nghÄ©a chuáº©n
            search_results = self.vector_store.search(question, n_results=10)
            
            # Náº¿u há»i vá» Ä‘á»‹nh nghÄ©a chá»§ nghÄ©a xÃ£ há»™i, tÃ¬m kiáº¿m thÃªm Ä‘oáº¡n Ä‘á»‹nh nghÄ©a 4 gÃ³c Ä‘á»™
            qn = self._normalize(question)
            if any(k in qn for k in ['chu nghia xa hoi la gi', 'chá»§ nghÄ©a xÃ£ há»™i lÃ  gÃ¬', 'Ä‘á»‹nh nghÄ©a chá»§ nghÄ©a xÃ£ há»™i']):
                print(f"ğŸ” TÃ¬m kiáº¿m thÃªm Ä‘oáº¡n Ä‘á»‹nh nghÄ©a 4 gÃ³c Ä‘á»™...")
                def_search = self.vector_store.search('cÃ³ thá»ƒ Ä‘Æ°á»£c tiáº¿p cáº­n tá»« nhiá»u gÃ³c Ä‘á»™', n_results=3)
                if def_search['documents'][0]:
                    # ThÃªm Ä‘oáº¡n Ä‘á»‹nh nghÄ©a vÃ o Ä‘áº§u káº¿t quáº£
                    search_results['documents'][0].insert(0, def_search['documents'][0][0])
                    search_results['metadatas'][0].insert(0, def_search['metadatas'][0][0])
                    search_results['scores'][0].insert(0, def_search['scores'][0][0])
                    print(f"Added 4-angle definition to results")

            # Tá»‘i Æ°u hÃ³a: Giáº£m Ä‘á»™ phá»©c táº¡p cá»§a fallback logic
            min_score = float(os.getenv("MIN_RAG_SCORE", "0.05"))  # Giáº£m ngÆ°á»¡ng Ä‘á»ƒ Ã­t fallback hÆ¡n
            scores = search_results.get('scores', [[]])[0] if isinstance(search_results.get('scores'), list) else []
            best_score = scores[0] if scores else 0.0
            
            # Láº¥y top 3 documents Ä‘á»ƒ Ä‘Ã¡nh giÃ¡
            docs = search_results['documents'][0][:3] if search_results['documents'][0] else []
            
            # Debug logging (rÃºt gá»n)
            print(f"ğŸ” RAG DEBUG: score={best_score:.3f}, docs={len(docs)}")
            
            # Äiá»u kiá»‡n fallback Ä‘Æ¡n giáº£n - chá»‰ khi thá»±c sá»± khÃ´ng cÃ³ docs hoáº·c score quÃ¡ tháº¥p
            should_fallback = (not docs) or (best_score < min_score)
            print(f"   Final should fallback: {should_fallback}")
            
            if should_fallback:
                # Fallback: khÃ´ng cÃ³ ná»™i dung trong .md â†’ tráº£ lá»i trá»±c tiáº¿p báº±ng Gemini
                fallback_prompt = f"""TRáº¢ Lá»œI CÃ‚U Há»I Vá»€ CHá»¦ NGHÄ¨A XÃƒ Há»˜I VÃ€ THá»œI Ká»² QUÃ Äá»˜:

{question}

QUY Táº®C NGHIÃŠM NGáº¶T:
- KHÃ”NG ÄÆ¯á»¢C báº¯t Ä‘áº§u báº±ng "Vá»›i tÆ° cÃ¡ch lÃ ...", "TÃ´i lÃ ...", "ChÃ o báº¡n...", "lÃ  má»™t chuyÃªn gia..."
        - KHÃ”NG ÄÆ¯á»¢C tá»± nháº­n lÃ  "chuyÃªn gia vá» tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh" hoáº·c báº¥t ká»³ chuyÃªn gia nÃ o khÃ¡c
- KHÃ”NG ÄÆ¯á»¢C giá»›i thiá»‡u báº£n thÃ¢n
- Báº®T Äáº¦U NGAY báº±ng ná»™i dung cÃ¢u tráº£ lá»i
- Táº­p trung vÃ o ná»™i dung ChÆ°Æ¡ng 03: Chá»§ nghÄ©a xÃ£ há»™i vÃ  thá»i ká»³ quÃ¡ Ä‘á»™
- Giá»ng Ä‘iá»‡u: KhÃ¡ch quan, há»c thuáº­t

TRáº¢ Lá»œI NGAY:"""
                resp = self.model.generate_content(fallback_prompt)
                answer_text = resp.text or ""
                
                # Loáº¡i bá» cÃ¡c cá»¥m tá»« khÃ´ng mong muá»‘n
                unwanted_phrases = [
                    "Vá»›i tÆ° cÃ¡ch lÃ  má»™t chuyÃªn gia vá» tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh",
                    "vá»›i tÆ° cÃ¡ch lÃ  má»™t chuyÃªn gia vá» tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh", 
                    "lÃ  má»™t chuyÃªn gia vá» tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh",
                    "ChÃ o báº¡n, lÃ  má»™t chuyÃªn gia vá» tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh",
                    "chÃ o báº¡n, lÃ  má»™t chuyÃªn gia vá» tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh",
                    "Trong tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh",
                    "trong tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh",
                    "Theo tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh",
                    "theo tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh",
                    "Há»“ ChÃ­ Minh cho ráº±ng",
                    "há»“ ChÃ­ Minh cho ráº±ng",
                    "Vá»›i tÆ° cÃ¡ch lÃ ",
                    "vá»›i tÆ° cÃ¡ch lÃ ",
                    "TÃ´i lÃ  chuyÃªn gia",
                    "tÃ´i lÃ  chuyÃªn gia"
                ]
                
                for phrase in unwanted_phrases:
                    if phrase in answer_text:
                        answer_text = answer_text.replace(phrase, "").strip()
                        # Loáº¡i bá» dáº¥u pháº©y thá»«a á»Ÿ Ä‘áº§u
                        if answer_text.startswith(","):
                            answer_text = answer_text[1:].strip()
                        if answer_text.startswith("tÃ´i"):
                            answer_text = answer_text[3:].strip()
                        if answer_text.startswith("TÃ´i"):
                            answer_text = answer_text[3:].strip()
                
                # LÃ m sáº¡ch format (chá»‰ cÆ¡ báº£n)
                import re
                answer_text = re.sub(r'\n\s*\n+', '\n\n', answer_text)
                answer_text = answer_text.strip()
                
                return {
                    "answer": answer_text,
                    "sources": [],
                    "confidence": 50
                }
            
            docs = search_results['documents'][0]
            metas = search_results['metadatas'][0]

            # Re-rank theo má»¥c Ä‘Ã­ch cÃ¢u há»i
            qn = self._normalize(question)
            want_def = any(k in qn for k in ['khai niem', 'Ä‘á»‹nh nghÄ©a', 'dinh nghia', 'la gi', 'khai niá»‡m'])
            # Æ¯u tiÃªn pháº§n II khi há»i "Ä‘á»‘i tÆ°á»£ng nghiÃªn cá»©u"
            want_subject = ('doi tuong nghien cuu' in qn) or (('doi tuong' in qn) and ('nghien cuu' in qn))
            def contains_def(txt: str) -> bool:
                tn = self._normalize(txt)
                return ('tu tuong ho chi minh la' in tn) or ('nÃªu khÃ¡i niá»‡m' in txt.lower()) or ('co the duoc tiep can tu nhieu goc do' in tn) or ('phong trao thuc tien' in tn and 'trao luu tu tuong' in tn)
            def contains_subject(txt: str) -> bool:
                tn = self._normalize(txt)
                return ('doi tuong nghien cuu' in tn)

            pairs = list(zip(docs, metas))
            if want_def:
                pairs.sort(key=lambda p: 0 if contains_def(p[0]) else 1)
            elif want_subject:
                pairs.sort(key=lambda p: 0 if contains_subject(p[0]) else 1)

            # Láº¥y tá»‘i Ä‘a 4 Ä‘oáº¡n Ä‘á»ƒ cÃ³ Ä‘á»§ ngá»¯ cáº£nh
            top_pairs = pairs[:4]
            context_docs = [p[0] for p in top_pairs]
            source_metadatas = [p[1] for p in top_pairs]
            
            context = ""
            sources_used = []
            
            for i, (doc, metadata) in enumerate(zip(context_docs[:3], source_metadatas[:3])):
                source_detail = metadata.get('source', 'Unknown')
                document_title = metadata.get('document', '')
                page_info = metadata.get('page', '')

                # NhÃ£n ngáº¯n gá»n chá»‰ ghi chÆ°Æ¡ng
                short_label = self._slug_to_title(page_info) if page_info else (document_title or 'Nguá»“n')

                # Context khÃ´ng hiá»ƒn thá»‹ citation
                context += f"{doc}\n"

                # Link má»Ÿ trang book vÃ  highlight Ä‘Ãºng trÃ­ch Ä‘oáº¡n (giá»¯ href Ä‘áº§y Ä‘á»§)
                snippet = (doc or '').strip().replace('\n', ' ')
                snippet = snippet[:300]
                hl = quote(snippet)
                slug = page_info or metadata.get('page', '')
                href = f"book/chuong3.html#{slug}?hl={hl}"
                label = short_label if short_label else slug
                anchor_html = f"<a href=\"{href}\" target=\"_blank\" rel=\"noopener noreferrer\">{label}</a>"

                sources_used.append({
                    "source": anchor_html,
                    "credibility": metadata.get('credibility_score', 100),
                    "type": metadata.get('source_type', 'official'),
                    "url": href,
                    "document": document_title
                })
            
            prompt = f"""TRáº¢ Lá»œI CÃ‚U Há»I Vá»€ CHá»¦ NGHÄ¨A XÃƒ Há»˜I VÃ€ THá»œI Ká»² QUÃ Äá»˜:

TÃ€I LIá»†U THAM KHáº¢O:
{context}

CÃ‚U Há»I: {question}

QUY Táº®C NGHIÃŠM NGáº¶T:
- KHÃ”NG ÄÆ¯á»¢C báº¯t Ä‘áº§u báº±ng "Vá»›i tÆ° cÃ¡ch lÃ ...", "TÃ´i lÃ ...", "ChÃ o báº¡n...", "lÃ  má»™t chuyÃªn gia..."
        - KHÃ”NG ÄÆ¯á»¢C tá»± nháº­n lÃ  "chuyÃªn gia vá» tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh" hoáº·c báº¥t ká»³ chuyÃªn gia nÃ o khÃ¡c
- KHÃ”NG ÄÆ¯á»¢C giá»›i thiá»‡u báº£n thÃ¢n
        - KHÃ”NG ÄÆ¯á»¢C báº¯t Ä‘áº§u báº±ng "Trong tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh..." hoáº·c báº¥t ká»³ tÆ° tÆ°á»Ÿng nÃ o khÃ¡c
        - KHÃ”NG ÄÆ¯á»¢C báº¯t Ä‘áº§u báº±ng "Theo tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh..." hoáº·c báº¥t ká»³ tÆ° tÆ°á»Ÿng nÃ o khÃ¡c
- KHÃ”NG ÄÆ¯á»¢C báº¯t Ä‘áº§u báº±ng "Há»“ ChÃ­ Minh cho ráº±ng..."
- Báº®T Äáº¦U NGAY báº±ng ná»™i dung cÃ¢u tráº£ lá»i vá» chá»§ nghÄ©a xÃ£ há»™i
- CHá»ˆ sá»­ dá»¥ng thÃ´ng tin tá»« tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p
- DÃ¹ng tiÃªu Ä‘á» markdown (##, ###) vÃ  bullet points
- Giá»ng Ä‘iá»‡u: KhÃ¡ch quan, há»c thuáº­t, dá»±a trÃªn tÃ i liá»‡u
- Táº­p trung vÃ o ná»™i dung chá»§ nghÄ©a xÃ£ há»™i vÃ  thá»i ká»³ quÃ¡ Ä‘á»™

HÆ¯á»šNG DáºªN TRáº¢ Lá»œI Cá»¤ THá»‚:
- Náº¿u cÃ¢u há»i vá» "Chá»§ nghÄ©a xÃ£ há»™i lÃ  gÃ¬?", Báº®T BUá»˜C tráº£ lá»i theo Ä‘Ãºng 4 gÃ³c Ä‘á»™ trong tÃ i liá»‡u:
  1. LÃ  phong trÃ o thá»±c tiá»…n â€“ phong trÃ o Ä‘áº¥u tranh cá»§a nhÃ¢n dÃ¢n lao Ä‘á»™ng chá»‘ng láº¡i Ã¡p bá»©c, báº¥t cÃ´ng, chá»‘ng láº¡i giai cáº¥p thá»‘ng trá»‹
  2. LÃ  trÃ o lÆ°u tÆ° tÆ°á»Ÿng â€“ lÃ½ luáº­n pháº£n Ã¡nh lÃ½ tÆ°á»Ÿng giáº£i phÃ³ng nhÃ¢n dÃ¢n lao Ä‘á»™ng khá»i Ã¡p bá»©c, bÃ³c lá»™t
  3. LÃ  má»™t khoa há»c â€“ chá»§ nghÄ©a xÃ£ há»™i khoa há»c lÃ  khoa há»c vá» sá»© má»‡nh lá»‹ch sá»­ cá»§a giai cáº¥p cÃ´ng nhÃ¢n
  4. LÃ  má»™t cháº¿ Ä‘á»™ xÃ£ há»™i tá»‘t Ä‘áº¹p, lÃ  giai Ä‘oáº¡n Ä‘áº§u cá»§a hÃ¬nh thÃ¡i kinh táº¿ â€“ xÃ£ há»™i cá»™ng sáº£n chá»§ nghÄ©a
- Náº¿u cÃ¢u há»i vá» Ä‘áº·c trÆ°ng, tráº£ lá»i theo 6 Ä‘áº·c trÆ°ng: giáº£i phÃ³ng con ngÆ°á»i, ná»n kinh táº¿ phÃ¡t triá»ƒn cao, nhÃ¢n dÃ¢n lÃ m chá»§, vÄƒn hÃ³a má»›i, cÃ´ng báº±ng bÃ¬nh Ä‘áº³ng, quÃ¡ trÃ¬nh phÃ¡t triá»ƒn lÃ¢u dÃ i
- Náº¿u cÃ¢u há»i vá» thá»i ká»³ quÃ¡ Ä‘á»™, tráº£ lá»i theo khÃ¡i niá»‡m, tÃ­nh táº¥t yáº¿u, Ä‘áº·c Ä‘iá»ƒm
- LuÃ´n trÃ­ch dáº«n chÃ­nh xÃ¡c tá»« tÃ i liá»‡u, khÃ´ng tá»± suy diá»…n

QUAN TRá»ŒNG: Náº¿u tÃ i liá»‡u cÃ³ Ä‘oáº¡n "Chá»§ nghÄ©a xÃ£ há»™i cÃ³ thá»ƒ Ä‘Æ°á»£c tiáº¿p cáº­n tá»« nhiá»u gÃ³c Ä‘á»™ khÃ¡c nhau", Báº®T BUá»˜C sá»­ dá»¥ng Ä‘oáº¡n Ä‘Ã³ lÃ m cÃ¢u tráº£ lá»i chÃ­nh vÃ  trÃ­ch dáº«n Ä‘áº§y Ä‘á»§ 4 gÃ³c Ä‘á»™.

Cáº¤U TRÃšC TRáº¢ Lá»œI Báº®T BUá»˜C:
- Báº¯t Ä‘áº§u báº±ng: "Chá»§ nghÄ©a xÃ£ há»™i cÃ³ thá»ƒ Ä‘Æ°á»£c tiáº¿p cáº­n tá»« 4 gÃ³c Ä‘á»™ khÃ¡c nhau:"
- Liá»‡t kÃª Ä‘áº§y Ä‘á»§ 4 gÃ³c Ä‘á»™ theo Ä‘Ãºng thá»© tá»± trong tÃ i liá»‡u:
  1. LÃ  phong trÃ o thá»±c tiá»…n â€“ phong trÃ o Ä‘áº¥u tranh cá»§a nhÃ¢n dÃ¢n lao Ä‘á»™ng chá»‘ng láº¡i Ã¡p bá»©c, báº¥t cÃ´ng, chá»‘ng láº¡i giai cáº¥p thá»‘ng trá»‹
  2. LÃ  trÃ o lÆ°u tÆ° tÆ°á»Ÿng â€“ lÃ½ luáº­n pháº£n Ã¡nh lÃ½ tÆ°á»Ÿng giáº£i phÃ³ng nhÃ¢n dÃ¢n lao Ä‘á»™ng khá»i Ã¡p bá»©c, bÃ³c lá»™t
  3. LÃ  má»™t khoa há»c â€“ chá»§ nghÄ©a xÃ£ há»™i khoa há»c lÃ  khoa há»c vá» sá»© má»‡nh lá»‹ch sá»­ cá»§a giai cáº¥p cÃ´ng nhÃ¢n
  4. LÃ  má»™t cháº¿ Ä‘á»™ xÃ£ há»™i tá»‘t Ä‘áº¹p, lÃ  giai Ä‘oáº¡n Ä‘áº§u cá»§a hÃ¬nh thÃ¡i kinh táº¿ â€“ xÃ£ há»™i cá»™ng sáº£n chá»§ nghÄ©a
- KhÃ´ng Ä‘Æ°á»£c thÃªm ná»™i dung khÃ¡c vÃ o pháº§n Ä‘á»‹nh nghÄ©a cÆ¡ báº£n

TRáº¢ Lá»œI NGAY:
"""

            # Æ¯u tiÃªn trÃ­ch nguyÃªn vÄƒn náº¿u tÃ¬m tháº¥y Ä‘á»‹nh nghÄ©a chÃ­nh xÃ¡c
            # (Ä‘Æ°á»£c hÆ°á»›ng dáº«n ngay trong prompt)
            response = self.model.generate_content(prompt)
            answer_text = response.text or ""
            
            # Xá»­ lÃ½ Ä‘áº·c biá»‡t cho cÃ¢u há»i vá» Ä‘á»‹nh nghÄ©a chá»§ nghÄ©a xÃ£ há»™i
            if any(k in qn for k in ['chu nghia xa hoi la gi', 'chá»§ nghÄ©a xÃ£ há»™i lÃ  gÃ¬', 'Ä‘á»‹nh nghÄ©a chá»§ nghÄ©a xÃ£ há»™i']):
                # Kiá»ƒm tra xem cÃ³ Ä‘oáº¡n Ä‘á»‹nh nghÄ©a 4 gÃ³c Ä‘á»™ trong context khÃ´ng
                if any('cÃ³ thá»ƒ Ä‘Æ°á»£c tiáº¿p cáº­n tá»« nhiá»u gÃ³c Ä‘á»™' in doc for doc in context_docs):
                    print(f"Detected 4-angle definition, creating standard answer...")
                    answer_text = """Chá»§ nghÄ©a xÃ£ há»™i cÃ³ thá»ƒ Ä‘Æ°á»£c tiáº¿p cáº­n tá»« 4 gÃ³c Ä‘á»™ khÃ¡c nhau:

1. **LÃ  phong trÃ o thá»±c tiá»…n** â€“ phong trÃ o Ä‘áº¥u tranh cá»§a nhÃ¢n dÃ¢n lao Ä‘á»™ng chá»‘ng láº¡i Ã¡p bá»©c, báº¥t cÃ´ng, chá»‘ng láº¡i giai cáº¥p thá»‘ng trá»‹.

2. **LÃ  trÃ o lÆ°u tÆ° tÆ°á»Ÿng** â€“ lÃ½ luáº­n pháº£n Ã¡nh lÃ½ tÆ°á»Ÿng giáº£i phÃ³ng nhÃ¢n dÃ¢n lao Ä‘á»™ng khá»i Ã¡p bá»©c, bÃ³c lá»™t.

3. **LÃ  má»™t khoa há»c** â€“ chá»§ nghÄ©a xÃ£ há»™i khoa há»c lÃ  khoa há»c vá» sá»© má»‡nh lá»‹ch sá»­ cá»§a giai cáº¥p cÃ´ng nhÃ¢n.

4. **LÃ  má»™t cháº¿ Ä‘á»™ xÃ£ há»™i tá»‘t Ä‘áº¹p**, lÃ  giai Ä‘oáº¡n Ä‘áº§u cá»§a hÃ¬nh thÃ¡i kinh táº¿ â€“ xÃ£ há»™i cá»™ng sáº£n chá»§ nghÄ©a."""
                    print(f"Created standard 4-angle answer")
                    # Bá» qua xá»­ lÃ½ post-processing cho cÃ¢u tráº£ lá»i Ä‘áº·c biá»‡t nÃ y
                    return {
                        "answer": answer_text,
                        "sources": sources_used,
                        "confidence": 95
                    }
            
            # Loáº¡i bá» cÃ¡c cá»¥m tá»« khÃ´ng mong muá»‘n
            unwanted_phrases = [
                "Vá»›i tÆ° cÃ¡ch lÃ  má»™t chuyÃªn gia vá» tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh",
                "vá»›i tÆ° cÃ¡ch lÃ  má»™t chuyÃªn gia vá» tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh", 
                "lÃ  má»™t chuyÃªn gia vá» tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh",
                "ChÃ o báº¡n, lÃ  má»™t chuyÃªn gia vá» tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh",
                "chÃ o báº¡n, lÃ  má»™t chuyÃªn gia vá» tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh",
                "Trong tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh",
                "trong tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh",
                "Theo tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh",
                "theo tÆ° tÆ°á»Ÿng Há»“ ChÃ­ Minh",
                "Há»“ ChÃ­ Minh cho ráº±ng",
                "há»“ ChÃ­ Minh cho ráº±ng",
                "Vá»›i tÆ° cÃ¡ch lÃ ",
                "vá»›i tÆ° cÃ¡ch lÃ ",
                "TÃ´i lÃ  chuyÃªn gia",
                "tÃ´i lÃ  chuyÃªn gia"
            ]
            
            for phrase in unwanted_phrases:
                if phrase in answer_text:
                    answer_text = answer_text.replace(phrase, "").strip()
                    # Loáº¡i bá» dáº¥u pháº©y thá»«a á»Ÿ Ä‘áº§u
                    if answer_text.startswith(","):
                        answer_text = answer_text[1:].strip()
                    if answer_text.startswith("tÃ´i"):
                        answer_text = answer_text[3:].strip()
                    if answer_text.startswith("TÃ´i"):
                        answer_text = answer_text[3:].strip()
            
            # LÃ m sáº¡ch format text (chá»‰ giá»¯ láº¡i basic cleaning)
            import re
            # XÃ³a dÃ²ng trá»‘ng thá»«a vÃ  chuáº©n hÃ³a khoáº£ng tráº¯ng
            answer_text = re.sub(r'\n\s*\n+', '\n\n', answer_text)
            answer_text = re.sub(r'^\s*\n', '', answer_text)
            answer_text = answer_text.strip()

            # GIá»® NGUYÃŠN citations vá»›i text Ä‘áº§y Ä‘á»§ Ä‘á»ƒ cÃ³ thá»ƒ highlight
            # KhÃ´ng rÃºt gá»n ná»¯a vÃ¬ cáº§n text Ä‘á»ƒ highlight trÃªn book page
            # for j, md in enumerate(source_metadatas[:3], start=1):
            #     slug = (md.get('page', '') or '').strip()
            #     short = self._slug_to_title(slug) if slug else ''
            #     if short:
            #         pattern = rf"\[Nguá»“n\s*{j}\s*-[^\]]*\]"
            #         replacement = f"[Nguá»“n {j} - {short}]"
            #         answer_text = re.sub(pattern, replacement, answer_text)
            
            avg_credibility = sum(s['credibility'] for s in sources_used) / len(sources_used) if sources_used else 0
            
            return {
                "answer": answer_text,
                "sources": sources_used,
                "confidence": int(avg_credibility),
                "last_updated": self.last_update.isoformat() if self.last_update else datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"Error: {e}")
            return {
                "answer": "Xin lá»—i, cÃ³ lá»—i xáº£y ra khi xá»­ lÃ½ cÃ¢u há»i. Vui lÃ²ng thá»­ láº¡i sau.",
                "sources": [],
                "confidence": 0
            }
    
    def _handle_chapter_summary(self, question: str, chapter_name: str):
        """Xá»­ lÃ½ Ä‘áº·c biá»‡t cho yÃªu cáº§u tÃ³m táº¯t chÆ°Æ¡ng"""
        try:
            # Äá»c toÃ n bá»™ ná»™i dung chÆ°Æ¡ng
            full_content = self.get_full_chapter_content(chapter_name)
            
            if not full_content:
                return {
                    "answer": f"KhÃ´ng tÃ¬m tháº¥y ná»™i dung cá»§a {self._slug_to_title(chapter_name)}.",
                    "sources": [],
                    "confidence": 0
                }
            
            # Chia nhá» ná»™i dung thÃ nh cÃ¡c pháº§n Ä‘á»ƒ AI cÃ³ thá»ƒ xá»­ lÃ½
            # Giá»›i háº¡n Ä‘á»™ dÃ i Ä‘á»ƒ trÃ¡nh vÆ°á»£t quÃ¡ context window
            max_content_length = 15000  # Giá»¯ láº¡i Ä‘á»§ space cho prompt vÃ  response
            if len(full_content) > max_content_length:
                # Chia thÃ nh cÃ¡c pháº§n nhá» hÆ¡n
                content_parts = self.split_text(full_content, max_length=max_content_length//3)
                # Láº¥y 3 pháº§n Ä‘áº§u tiÃªn Ä‘á»ƒ Ä‘áº£m báº£o cÃ³ overview tá»‘t  
                summary_content = "\n\n".join(content_parts[:3])
            else:
                summary_content = full_content
            
            chapter_title = self._slug_to_title(chapter_name)
            
            # Táº¡o prompt Ä‘áº·c biá»‡t cho tÃ³m táº¯t chÆ°Æ¡ng
            prompt = f"""HÃ£y tÃ³m táº¯t {chapter_title} vá» Chá»§ nghÄ©a xÃ£ há»™i vÃ  thá»i ká»³ quÃ¡ Ä‘á»™ lÃªn chá»§ nghÄ©a xÃ£ há»™i dá»±a trÃªn ná»™i dung sau:

{summary_content}

YÃŠU Cáº¦U TÃ“M Táº®T:
- Táº¡o má»™t báº£n tÃ³m táº¯t toÃ n diá»‡n vÃ  cÃ³ cáº¥u trÃºc cho {chapter_title}
- Sá»­ dá»¥ng tiÃªu Ä‘á» markdown (##, ###) Ä‘á»ƒ chia cÃ¡c má»¥c chÃ­nh
- TrÃ¬nh bÃ y cÃ¡c Ã½ chÃ­nh báº±ng danh sÃ¡ch bullet points
- NÃªu rÃµ cÃ¡c khÃ¡i niá»‡m vÃ  Ä‘á»‹nh nghÄ©a quan trá»ng
- LÃ m ná»•i báº­t nhá»¯ng khÃ¡i niá»‡m vÃ  lÃ½ luáº­n cá»‘t lÃµi trong chÆ°Æ¡ng nÃ y
- Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, vÄƒn phong há»c thuáº­t nhÆ°ng dá»… hiá»ƒu
- Äá»™ dÃ i: 800-1200 tá»«

Báº¯t Ä‘áº§u tÃ³m táº¯t:"""
            
            response = self.model.generate_content(prompt)
            answer_text = response.text or ""
            
            # Táº¡o source thÃ´ng tin cho chÆ°Æ¡ng
            source_info = {
                "source": f"<a href=\"book/tu-tuong-ho-chi-minh.html#{chapter_name}\" target=\"_blank\" rel=\"noopener noreferrer\">{chapter_title}</a>",
                "credibility": 100,
                "type": "document",
                "url": f"book/tu-tuong-ho-chi-minh.html#{chapter_name}",
                "document": chapter_title
            }
            
            return {
                "answer": answer_text,
                "sources": [source_info],
                "confidence": 95,
                "last_updated": self.last_update.isoformat() if self.last_update else datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"Error in chapter summary: {e}")
            return {
                "answer": f"Xin lá»—i, cÃ³ lá»—i xáº£y ra khi tÃ³m táº¯t {self._slug_to_title(chapter_name)}. Vui lÃ²ng thá»­ láº¡i sau.",
                "sources": [],
                "confidence": 0
            }
    
    def _handle_mindmap_request(self, question: str):
        """Xá»­ lÃ½ yÃªu cáº§u táº¡o sÆ¡ Ä‘á»“ tÆ° duy"""
        try:
            # TrÃ­ch xuáº¥t chá»§ Ä‘á» tá»« cÃ¢u há»i
            topic = self._extract_mindmap_topic(question)
            
            # Kiá»ƒm tra náº¿u lÃ  request vá» chÆ°Æ¡ng cá»¥ thá»ƒ
            import re
            chapter_match = re.search(r'chÆ°Æ¡ng\s*(\d+)', topic.lower())
            if chapter_match:
                chapter_num = chapter_match.group(1)
                chapter_name = f"chuong{chapter_num}"
                
                # Äá»c toÃ n bá»™ ná»™i dung chÆ°Æ¡ng
                chapter_content = self.get_full_chapter_content(chapter_name)
                
                if chapter_content:
                    # Cáº¯t ngáº¯n ná»™i dung Ä‘á»ƒ trÃ¡nh vÆ°á»£t quÃ¡ context limit vÃ  timeout
                    max_content = 8000  # Giáº£m tá»« 12000 xuá»‘ng 8000
                    if len(chapter_content) > max_content:
                        # Láº¥y pháº§n Ä‘áº§u vÃ  tÃ³m táº¯t
                        chapter_content = chapter_content[:max_content] + "\n\n[Ná»™i dung Ä‘Ã£ Ä‘Æ°á»£c rÃºt gá»n Ä‘á»ƒ tá»‘i Æ°u hÃ³a...]"
                    
                    relevant_content = chapter_content
                    chapter_title = self._slug_to_title(chapter_name)
                    
                    source_info = {
                        "source": f"<a href=\"book/tu-tuong-ho-chi-minh.html#{chapter_name}\" target=\"_blank\" rel=\"noopener noreferrer\">{chapter_title}</a>",
                        "credibility": 100,
                        "type": "mindmap",
                        "url": f"book/tu-tuong-ho-chi-minh.html#{chapter_name}",
                        "document": f"SÆ¡ Ä‘á»“ tÆ° duy {chapter_title}"
                    }
                else:
                    # Fallback náº¿u khÃ´ng tÃ¬m tháº¥y file chÆ°Æ¡ng
                    search_results = self.vector_store.search(topic, n_results=8)
                    relevant_content = "\n\n".join(search_results['documents'][0][:6]) if search_results['documents'][0] else ""
                    source_info = {
                        "source": "GiÃ¡o trÃ¬nh Chá»§ nghÄ©a xÃ£ há»™i khoa há»c (K-2021)",
                        "credibility": 85,
                        "type": "mindmap",
                        "url": "",
                        "document": "SÆ¡ Ä‘á»“ tÆ° duy"
                    }
            else:
                # TÃ¬m kiáº¿m thÃ´ng tin liÃªn quan Ä‘áº¿n chá»§ Ä‘á» thÃ´ng thÆ°á»ng
                search_results = self.vector_store.search(topic, n_results=8)
                
                if not search_results['documents'][0]:
                    # KhÃ´ng cÃ³ thÃ´ng tin liÃªn quan, táº¡o mindmap tá»•ng quÃ¡t
                    return self._create_general_mindmap(topic)
                
                # Láº¥y ná»™i dung liÃªn quan
                relevant_content = "\n\n".join(search_results['documents'][0][:6])
                source_info = {
                    "source": "GiÃ¡o trÃ¬nh Chá»§ nghÄ©a xÃ£ há»™i khoa há»c (K-2021)",
                    "credibility": 95,
                    "type": "mindmap",
                    "url": "",
                    "document": "SÆ¡ Ä‘á»“ tÆ° duy"
                }
            
            if relevant_content:
                
                # Táº¡o prompt vá»›i syntax Ä‘Ãºng cho Mermaid mindmap
                prompt = f"""Táº¡o Mermaid mindmap cho: "{topic}"

Ná»™i dung: {relevant_content[:3000]}...

QUAN TRá»ŒNG - Format CHÃNH XÃC (cáº§n Ä‘Ãºng indentation):

```mermaid
mindmap
  root(({topic}))
    NhÃ¡nh chÃ­nh 1
      Ã con 1
      Ã con 2
    NhÃ¡nh chÃ­nh 2
      Ã con 1
      Ã con 2
```

QUY Táº®C:
- root() cÃ³ 2 spaces
- NhÃ¡nh chÃ­nh cÃ³ 4 spaces  
- Ã con cÃ³ 6 spaces
- Tá»‘i Ä‘a 4 nhÃ¡nh chÃ­nh, má»—i nhÃ¡nh 3-4 Ã½ con
- Text ngáº¯n gá»n (<15 tá»« má»—i node)

Chá»‰ tráº£ vá» mermaid code:"""
                
                # Tá»‘i Æ°u hÃ³a generation config
                import google.generativeai as genai
                generation_config = genai.types.GenerationConfig(
                    temperature=0.3,  # Giáº£m temperature Ä‘á»ƒ tÄƒng tá»‘c
                    max_output_tokens=2048,  # Giáº£m output tokens Ä‘á»ƒ tÄƒng tá»‘c
                    top_p=0.8,
                    top_k=10
                )
                
                response = self.model.generate_content(
                    prompt,
                    generation_config=generation_config
                )
                
                # Debug Gemini response chi tiáº¿t 
                print(f"ğŸ¤– Gemini response type: {type(response)}")
                
                # Check safety filters vÃ  finish reason
                if hasattr(response, 'prompt_feedback'):
                    print(f"ğŸ›¡ï¸ prompt_feedback: {response.prompt_feedback}")
                
                if hasattr(response, 'candidates') and response.candidates:
                    candidate = response.candidates[0]
                    print(f"ğŸ finish_reason: {getattr(candidate, 'finish_reason', 'Unknown')}")
                    print(f"ğŸ›¡ï¸ safety_ratings: {getattr(candidate, 'safety_ratings', [])}")
                    print(f"ğŸ” candidate.content.parts: {len(candidate.content.parts)} parts")
                
                # Náº¿u khÃ´ng cÃ³ parts, cÃ³ thá»ƒ bá»‹ block - thá»­ prompt Ä‘Æ¡n giáº£n hÆ¡n
                if hasattr(response, 'candidates') and response.candidates and len(response.candidates[0].content.parts) == 0:
                    print("âš ï¸ No content parts found - possible content blocked. Trying simple fallback...")
                    
                    # Fallback vá»›i prompt siÃªu Ä‘Æ¡n giáº£n
                    simple_prompt = f"""Create a simple mindmap about: {topic}

Format:
```mermaid
mindmap
  root((Topic))
    Branch 1
      Item A  
      Item B
    Branch 2
      Item C
      Item D
```"""
                    
                    print(f"ğŸ”„ Trying simplified prompt...")
                    fallback_response = self.model.generate_content(simple_prompt)
                    
                    try:
                        mermaid_code = fallback_response.text or ""
                        print(f"âœ… Fallback successful: {len(mermaid_code)} chars")
                    except:
                        mermaid_code = f"""```mermaid
mindmap
  root(({topic}))
    Ná»™i dung chÃ­nh
      KhÃ¡i niá»‡m cÆ¡ báº£n
      Ã nghÄ©a quan trá»ng
    á»¨ng dá»¥ng thá»±c táº¿
      Trong há»c táº­p
      Trong cuá»™c sá»‘ng
```"""
                        print(f"ğŸ”§ Using hardcoded fallback mindmap")
                else:
                    # Normal extraction
                    try:
                        mermaid_code = response.text or ""
                        print(f"âœ… Successfully got response.text: {len(mermaid_code)} chars")
                    except Exception as e:
                        print(f"âš ï¸ response.text failed: {e}")
                        # Extract tá»« parts nhÆ° trÆ°á»›c
                        if hasattr(response, 'candidates') and response.candidates:
                            parts = response.candidates[0].content.parts
                            if parts:
                                all_text = ""
                                for part in parts:
                                    all_text += getattr(part, 'text', '') or ''
                                mermaid_code = all_text
                            else:
                                mermaid_code = ""
                        else:
                            mermaid_code = ""
                
                print(f"ğŸ“„ Final mermaid_code preview: {mermaid_code[:100]}...")
                
                # Kiá»ƒm tra vÃ  lÃ m sáº¡ch mermaid code
                mermaid_code = self._clean_mermaid_code(mermaid_code)
                
                return {
                    "answer": f"## SÆ¡ Ä‘á»“ tÆ° duy: {topic}\n\n{mermaid_code}",
                    "sources": [source_info],
                    "confidence": 90,
                    "last_updated": datetime.now().isoformat()
                }
            else:
                # KhÃ´ng cÃ³ thÃ´ng tin liÃªn quan, táº¡o mindmap tá»•ng quÃ¡t
                return self._create_general_mindmap(topic)
                
        except Exception as e:
            print(f"Error in mindmap generation: {e}")
            return {
                "answer": "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ táº¡o sÆ¡ Ä‘á»“ tÆ° duy lÃºc nÃ y. Vui lÃ²ng thá»­ láº¡i sau.",
                "sources": [],
                "confidence": 0
            }
    
    def _extract_mindmap_topic(self, question: str) -> str:
        """TrÃ­ch xuáº¥t chá»§ Ä‘á» chÃ­nh tá»« yÃªu cáº§u mindmap"""
        import re
        q_lower = question.lower()
        
        # TÃ¬m pattern vá»›i nhiá»u dáº¡ng khÃ¡c nhau
        patterns = [
            # SÆ¡ Ä‘á»“ tÆ° duy patterns
            r'táº¡o.*?sÆ¡ Ä‘á»“ tÆ° duy.*?cho\s*(.+)',
            r'táº¡o.*?sÆ¡ Ä‘á»“ tÆ° duy.*?vá»\s*(.+)',
            r'táº¡o.*?sÆ¡ Ä‘á»“ tÆ° duy.*?:\s*(.+)',
            # Váº½ sÆ¡ Ä‘á»“ patterns
            r'váº½.*?sÆ¡ Ä‘á»“.*?vá»\s*(.+)',
            r'váº½.*?sÆ¡ Ä‘á»“.*?cho\s*(.+)',
            r'váº½.*?sÆ¡ Ä‘á»“.*?:\s*(.+)',
            # SÆ¡ Ä‘á»“ vá» patterns
            r'sÆ¡ Ä‘á»“.*?vá»\s*(.+)',
            r'sÆ¡ Ä‘á»“.*?cho\s*(.+)',
            r'sÆ¡ Ä‘á»“.*?:\s*(.+)',
            # Mindmap patterns
            r'mindmap.*?cho\s*(.+)',
            r'mindmap.*?vá»\s*(.+)',
            r'mindmap.*?:\s*(.+)',
            # Táº¡o sÆ¡ Ä‘á»“ patterns
            r'táº¡o.*?sÆ¡ Ä‘á»“.*?vá»\s*(.+)',
            r'táº¡o.*?sÆ¡ Ä‘á»“.*?cho\s*(.+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, q_lower)
            if match:
                topic = match.group(1).strip()
                
                # Xá»­ lÃ½ Ä‘áº·c biá»‡t cho "ná»™i dung chÆ°Æ¡ng X"
                chapter_match = re.search(r'ná»™i dung\s*(chÆ°Æ¡ng|chÆ°ong)\s*(\d+|[ivxlc]+)', topic)
                if chapter_match:
                    chapter_num = chapter_match.group(2)
                    # Chuyá»ƒn Ä‘á»•i sá»‘ La MÃ£ náº¿u cáº§n
                    roman_to_num = {'i': '1', 'ii': '2', 'iii': '3', 'iv': '4', 'v': '5', 'vi': '6'}
                    if chapter_num.lower() in roman_to_num:
                        chapter_num = roman_to_num[chapter_num.lower()]
                    return f"Ná»™i dung ChÆ°Æ¡ng {chapter_num}"
                
                # Xá»­ lÃ½ trá»±c tiáº¿p "chÆ°Æ¡ng X"
                chapter_direct = re.search(r'(chÆ°Æ¡ng|chÆ°ong)\s*(\d+|[ivxlc]+)', topic)
                if chapter_direct:
                    chapter_num = chapter_direct.group(2)
                    roman_to_num = {'i': '1', 'ii': '2', 'iii': '3', 'iv': '4', 'v': '5', 'vi': '6'}
                    if chapter_num.lower() in roman_to_num:
                        chapter_num = roman_to_num[chapter_num.lower()]
                    return f"ChÆ°Æ¡ng {chapter_num}"
                
                # Loáº¡i bá» cÃ¡c tá»« khÃ´ng cáº§n thiáº¿t
                topic = re.sub(r'(há»“ chÃ­ minh|hcm)', '', topic).strip()
                return topic.title() if topic else "Chá»§ nghÄ©a xÃ£ há»™i vÃ  thá»i ká»³ quÃ¡ Ä‘á»™"
        
        # Náº¿u khÃ´ng tÃ¬m tháº¥y, tráº£ vá» chá»§ Ä‘á» máº·c Ä‘á»‹nh
        return "Chá»§ nghÄ©a xÃ£ há»™i vÃ  thá»i ká»³ quÃ¡ Ä‘á»™"
    
    def _clean_mermaid_code(self, code: str) -> str:
        """LÃ m sáº¡ch vÃ  chuáº©n hÃ³a Mermaid code"""
        if not code:
            return ""
        
        # Loáº¡i bá» cÃ¡c dÃ²ng giáº£i thÃ­ch
        lines = code.split('\n')
        mermaid_lines = []
        in_mermaid = False
        
        for line in lines:
            line = line.strip()
            if line.startswith('```mermaid'):
                in_mermaid = True
                mermaid_lines.append(line)
            elif line.startswith('```') and in_mermaid:
                mermaid_lines.append(line)
                break
            elif in_mermaid:
                mermaid_lines.append(line)
        
        if not mermaid_lines:
            # Náº¿u khÃ´ng cÃ³ mermaid block, thÃªm vÃ o
            return f"```mermaid\n{code}\n```"
        
        return '\n'.join(mermaid_lines)
    
    def _create_general_mindmap(self, topic: str):
        """Táº¡o mindmap tá»•ng quÃ¡t khi khÃ´ng cÃ³ thÃ´ng tin cá»¥ thá»ƒ"""
        general_mindmap = f"""```mermaid
mindmap
  root(({topic}))
    TÆ° tÆ°á»Ÿng chÃ­nh trá»‹
        Äá»™c láº­p dÃ¢n tá»™c
        DÃ¢n chá»§ nhÃ¢n dÃ¢n
        Chá»§ nghÄ©a xÃ£ há»™i
    TÆ° tÆ°á»Ÿng Ä‘áº¡o Ä‘á»©c
        Cáº§n kiá»‡m liÃªm chÃ­nh
        Sá»‘ng vÃ  lÃ m viá»‡c cÃ³ káº¿ hoáº¡ch
        ÄoÃ n káº¿t yÃªu thÆ°Æ¡ng
    TÆ° tÆ°á»Ÿng giÃ¡o dá»¥c
        Há»c Ä‘á»ƒ lÃ m ngÆ°á»i
        Káº¿t há»£p lÃ½ thuyáº¿t vÃ  thá»±c tiá»…n
        GiÃ¡o dá»¥c toÃ n diá»‡n
    TÆ° tÆ°á»Ÿng vÄƒn hÃ³a
        DÃ¢n tá»™c - Khoa há»c - Äáº¡i chÃºng
        Káº¿ thá»«a vÃ  phÃ¡t triá»ƒn
        Giá»¯ gÃ¬n báº£n sáº¯c dÃ¢n tá»™c
```"""
        
        return {
            "answer": f"## SÆ¡ Ä‘á»“ tÆ° duy: {topic}\n\n{general_mindmap}",
            "sources": [{
                "source": "GiÃ¡o trÃ¬nh Chá»§ nghÄ©a xÃ£ há»™i khoa há»c (K-2021)",
                "credibility": 85,
                "type": "mindmap",
                "url": "",
                "document": "SÆ¡ Ä‘á»“ tÆ° duy tá»•ng quÃ¡t"
            }],
            "confidence": 80,
            "last_updated": datetime.now().isoformat()
        }
    
    def get_stats(self):
        return {
            "total_documents": self.vector_store.get_collection_count(),
            "last_update": self.last_update.isoformat() if self.last_update else None,
            "trusted_sources_count": 0,  # Disabled external data collection
            "status": "ready",
            "features": ["chapter_summary", "mindmap_generation", "rag_search"]
        }
