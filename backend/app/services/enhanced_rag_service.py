import google.generativeai as genai
from .vector_store import SimpleVectorStore
# from .web_data_collector import WebDataCollector  # Disabled to prevent external API calls
import os
from dotenv import load_dotenv
import json
from datetime import datetime
from typing import List
from urllib.parse import quote
import unicodedata
import re

load_dotenv()

class EnhancedRAGService:
    def __init__(self):
        self.vector_store = SimpleVectorStore()
        # self.data_collector = WebDataCollector()  # Disabled to prevent external API calls
        
        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
        self.model = genai.GenerativeModel('gemini-2.5-flash')
        
        self.last_update = None
        print("Enhanced RAG Service v2.1 ready with improved citations!")
    
    def add_chapter03_corpus(self):
        """Th√™m corpus Ch∆∞∆°ng 03: Ch·ªß nghƒ©a x√£ h·ªôi v√† th·ªùi k·ª≥ qu√° ƒë·ªô l√™n ch·ªß nghƒ©a x√£ h·ªôi"""
        # Load Chapter 03 content from file
        chapter03_path = os.path.join(os.path.dirname(__file__), "../../data/book/chuong3.md")
        
        if not os.path.exists(chapter03_path):
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file {chapter03_path}")
            return
        
        with open(chapter03_path, 'r', encoding='utf-8') as f:
            chapter03_content = f.read()
        
        # Split content into meaningful chunks
        comprehensive_docs = self._split_chapter03_content(chapter03_content)
        
        # Create metadata for Chapter 03 content
        comprehensive_metadata = []
        for i, doc in enumerate(comprehensive_docs):
            metadata = {
                "source": "Gi√°o tr√¨nh Ch·ªß nghƒ©a x√£ h·ªôi khoa h·ªçc (K-2021)",
                "document": "Ch∆∞∆°ng III: Ch·ªß nghƒ©a x√£ h·ªôi v√† th·ªùi k·ª≥ qu√° ƒë·ªô l√™n ch·ªß nghƒ©a x√£ h·ªôi",
                "topic": "ch·ªß nghƒ©a x√£ h·ªôi",
                "page": f"chunk_{i+1}",
                "credibility_score": 100,
                "source_type": "textbook"
            }
            comprehensive_metadata.append(metadata)
        
        self.vector_store.add_documents(comprehensive_docs, comprehensive_metadata)
        print(f"Added {len(comprehensive_docs)} documents from Chapter 03 with detailed citations")
    
    def _split_chapter03_content(self, content: str) -> List[str]:
        """Chia n·ªôi dung Ch∆∞∆°ng 03 th√†nh c√°c ƒëo·∫°n c√≥ √Ω nghƒ©a"""
        # Split by major sections
        sections = re.split(r'\n(?=[IVX]+\.)', content)
        
        chunks = []
        for section in sections:
            if not section.strip():
                continue
                
            # Further split by paragraphs
            paragraphs = section.split('\n\n')
            current_chunk = ""
            
            for paragraph in paragraphs:
                paragraph = paragraph.strip()
                if not paragraph:
                    continue
                    
                # If adding this paragraph would make chunk too long, save current chunk
                if len(current_chunk) + len(paragraph) > 1000 and current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = paragraph
                else:
                    if current_chunk:
                        current_chunk += "\n\n" + paragraph
                    else:
                        current_chunk = paragraph
            
            # Add remaining chunk
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
        
        return chunks

    def load_chapter03_data(self):
        """Load d·ªØ li·ªáu Ch∆∞∆°ng 03 v√†o vector store"""
        print("üîÑ ƒêang t·∫£i d·ªØ li·ªáu Ch∆∞∆°ng 03...")
        self.add_chapter03_corpus()
        print("‚úÖ Ho√†n th√†nh t·∫£i d·ªØ li·ªáu Ch∆∞∆°ng 03!")

    def ingest_markdown_folder(self, folder_path: str):
        """ƒê·ªçc t·∫•t c·∫£ c√°c file .md trong th∆∞ m·ª•c v√† ƒë∆∞a v√†o vector store.
        - M·ªçi citation s·∫Ω tr·ªè v·ªÅ 'Gi√°o tr√¨nh Ch·ªß nghƒ©a x√£ h·ªôi khoa h·ªçc'.
        - 'document' l√† t√™n file (kh√¥ng ƒëu√¥i), v√≠ d·ª•: 'chuong3' -> 'Ch∆∞∆°ng 03'.
        """
        try:
            if not os.path.exists(folder_path):
                os.makedirs(folder_path, exist_ok=True)
                print(f"T·∫°o th∆∞ m·ª•c book: {folder_path}")

            md_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.md')]
            if not md_files:
                print(f"Kh√¥ng t√¨m th·∫•y file .md trong {folder_path}")
                return

            all_docs, all_metas = [], []
            for fname in md_files:
                fpath = os.path.join(folder_path, fname)
                try:
                    with open(fpath, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                    if not content:
                        continue

                    # C·∫Øt nh·ªè n·ªôi dung ƒë·ªÉ index
                    chunks = self.split_text(content, max_length=700)
                    # T√™n hi·ªÉn th·ªã c·ªßa t√†i li·ªáu
                    base = os.path.splitext(fname)[0]
                    display_name = base.replace('-', ' ').title()
                    # Chu·∫©n h√≥a t√™n hi·ªÉn th·ªã v·ªõi d·∫•u ti·∫øng Vi·ªát cho c√°c trang ch√≠nh
                    bl = base.lower()
                    if bl == 'tu-tuong-ho-chi-minh':
                        display_name = 'Ch·ªß nghƒ©a x√£ h·ªôi v√† th·ªùi k·ª≥ qu√° ƒë·ªô'
                    elif bl == 'muc-luc':
                        display_name = 'M·ª•c l·ª•c'
                    elif bl in ('chuong1', 'chuong-1'):
                        display_name = 'Ch∆∞∆°ng I'
                    elif bl in ('chuong2', 'chuong-2'):
                        display_name = 'Ch∆∞∆°ng II'
                    elif bl in ('chuong3', 'chuong-3'):
                        display_name = 'Ch∆∞∆°ng III'
                    elif bl in ('chuong4', 'chuong-4'):
                        display_name = 'Ch∆∞∆°ng IV'
                    elif bl in ('chuong5', 'chuong-5'):
                        display_name = 'Ch∆∞∆°ng V'
                    elif bl in ('chuong6', 'chuong-6'):
                        display_name = 'Ch∆∞∆°ng VI'

                    for ch in chunks:
                        all_docs.append(ch)
                        all_metas.append({
                            "source": "Gi√°o tr√¨nh Ch·ªß nghƒ©a x√£ h·ªôi khoa h·ªçc (K-2021)",
                            "document": display_name,
                            "page": base,
                            "credibility_score": 95,
                            "source_type": "document",
                            "url": f"/book/{base}"
                        })
                except Exception as e:
                    print(f"Error reading {fname}: {e}")

            if all_docs:
                self.vector_store.add_documents(all_docs, all_metas)
                print(f"Ingested {len(all_docs)} segments from markdown folder {folder_path}")
        except Exception as e:
            print(f"Error ingesting markdown: {e}")
    
    def update_knowledge_base(self, force_update=False):
        """C·∫≠p nh·∫≠t knowledge base ch·ªâ t·ª´ Ch∆∞∆°ng 03: Ch·ªß nghƒ©a x√£ h·ªôi v√† th·ªùi k·ª≥ qu√° ƒë·ªô l√™n ch·ªß nghƒ©a x√£ h·ªôi.
        N·∫øu force_update=True: x√≥a index c≈© tr∆∞·ªõc khi ingest ƒë·ªÉ tr√°nh l·∫´n ngu·ªìn c≈©.
        """
        if force_update:
            self.vector_store.reset()
        self.load_chapter03_data()
        self.last_update = datetime.now()
        print("Knowledge base updated t·ª´ Ch∆∞∆°ng 03: Ch·ªß nghƒ©a x√£ h·ªôi v√† th·ªùi k·ª≥ qu√° ƒë·ªô l√™n ch·ªß nghƒ©a x√£ h·ªôi")
    
    def split_text(self, text: str, max_length: int = 700) -> List[str]:
        """Chia nh·ªè theo ƒëo·∫°n (paragraph-first) ƒë·ªÉ gi·ªØ nguy√™n c√°c kh·ªëi ƒë·ªãnh nghƒ©a/tr√≠ch d·∫´n.
        - ∆Øu ti√™n t√°ch theo 2+ d√≤ng tr·∫Øng.
        - N·∫øu ƒëo·∫°n qu√° d√†i, fallback t√°ch theo c√¢u '. '.
        """
        paras = [p.strip() for p in re.split(r"\n\s*\n+", text) if p.strip()]
        chunks: List[str] = []
        for p in paras:
            if len(p) <= max_length:
                chunks.append(p)
            else:
                sentences = p.split('. ')
                current = ""
                for s in sentences:
                    if len(current) + len(s) + 2 <= max_length:
                        current += (s + ". ")
                    else:
                        if current:
                            chunks.append(current.strip())
                        current = s + ". "
                if current:
                    chunks.append(current.strip())
        return chunks
    
    def _normalize(self, s: str) -> str:
        """Chu·∫©n h√≥a text: lo·∫°i b·ªè d·∫•u ti·∫øng Vi·ªát v√† chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng"""
        if not s:
            return ''
        
        # B·∫£ng chuy·ªÉn ƒë·ªïi k√Ω t·ª± c√≥ d·∫•u ti·∫øng Vi·ªát
        vietnamese_chars = {
            '√†': 'a', '√°': 'a', '·∫£': 'a', '√£': 'a', '·∫°': 'a',
            'ƒÉ': 'a', '·∫±': 'a', '·∫Ø': 'a', '·∫≥': 'a', '·∫µ': 'a', '·∫∑': 'a',
            '√¢': 'a', '·∫ß': 'a', '·∫•': 'a', '·∫©': 'a', '·∫´': 'a', '·∫≠': 'a',
            '√®': 'e', '√©': 'e', '·∫ª': 'e', '·∫Ω': 'e', '·∫π': 'e',
            '√™': 'e', '·ªÅ': 'e', '·∫ø': 'e', '·ªÉ': 'e', '·ªÖ': 'e', '·ªá': 'e',
            '√¨': 'i', '√≠': 'i', '·ªâ': 'i', 'ƒ©': 'i', '·ªã': 'i',
            '√≤': 'o', '√≥': 'o', '·ªè': 'o', '√µ': 'o', '·ªç': 'o',
            '√¥': 'o', '·ªì': 'o', '·ªë': 'o', '·ªï': 'o', '·ªó': 'o', '·ªô': 'o',
            '∆°': 'o', '·ªù': 'o', '·ªõ': 'o', '·ªü': 'o', '·ª°': 'o', '·ª£': 'o',
            '√π': 'u', '√∫': 'u', '·ªß': 'u', '≈©': 'u', '·ª•': 'u',
            '∆∞': 'u', '·ª´': 'u', '·ª©': 'u', '·ª≠': 'u', '·ªØ': 'u', '·ª±': 'u',
            '·ª≥': 'y', '√Ω': 'y', '·ª∑': 'y', '·ªπ': 'y', '·ªµ': 'y',
            'ƒë': 'd', 'ƒê': 'd'
        }
        
        # Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng
        s = s.lower()
        
        # Thay th·∫ø c√°c k√Ω t·ª± c√≥ d·∫•u
        for vn_char, latin_char in vietnamese_chars.items():
            s = s.replace(vn_char, latin_char)
        
        # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát, ch·ªâ gi·ªØ ch·ªØ v√† s·ªë
        s = re.sub(r'[^a-z0-9\s]', '', s)
        
        # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
        s = re.sub(r'\s+', ' ', s).strip()
        
        return s

    def _slug_to_title(self, slug: str) -> str:
        s = (slug or '').lower().strip()
        mapping = {
            'chuong1': 'Ch∆∞∆°ng I',
            'chuong2': 'Ch∆∞∆°ng II',
            'chuong3': 'Ch∆∞∆°ng III',
            'chuong4': 'Ch∆∞∆°ng IV',
            'chuong5': 'Ch∆∞∆°ng V',
            'chuong6': 'Ch∆∞∆°ng VI',
        }
        return mapping.get(s, slug or '')
    
    def detect_chapter_summary_request(self, question: str) -> tuple[bool, str]:
        """Ph√°t hi·ªán y√™u c·∫ßu t√≥m t·∫Øt ch∆∞∆°ng v√† tr·∫£ v·ªÅ (is_summary, chapter_name)"""
        q_norm = self._normalize(question)
        summary_keywords = ['tom tat', 'tom tac', 'tong ket', 'noi dung chinh', 'yeu to']
        chapter_keywords = ['chuong', 'phan']
        
        # Ki·ªÉm tra c√≥ t·ª´ kh√≥a t√≥m t·∫Øt
        has_summary = any(kw in q_norm for kw in summary_keywords)
        has_chapter = any(kw in q_norm for kw in chapter_keywords)
        
        if not (has_summary and has_chapter):
            return False, ""
        
        # T√¨m s·ªë ch∆∞∆°ng
        import re
        # T√¨m ch∆∞∆°ng b·∫±ng s·ªë La M√£ ho·∫∑c s·ªë Arabic
        chapter_match = re.search(r'ch∆∞∆°ng\s*(\d+|[ivxlcdm]+)', q_norm)
        if chapter_match:
            chapter_num = chapter_match.group(1)
            # Chuy·ªÉn s·ªë La M√£ th√†nh s·ªë Arabic n·∫øu c·∫ßn
            roman_to_num = {'i': '1', 'ii': '2', 'iii': '3', 'iv': '4', 'v': '5', 'vi': '6'}
            if chapter_num.lower() in roman_to_num:
                chapter_num = roman_to_num[chapter_num.lower()]
            return True, f"chuong{chapter_num}"
        
        # T√¨m theo pattern "ch∆∞∆°ng X"
        for i in range(1, 7):
            if f"chuong {i}" in q_norm or f"chuong{i}" in q_norm:
                return True, f"chuong{i}"
        
        return False, ""
    
    def detect_mindmap_request(self, question: str) -> bool:
        """Ph√°t hi·ªán y√™u c·∫ßu t·∫°o s∆° ƒë·ªì t∆∞ duy"""
        q_norm = self._normalize(question)
        mindmap_keywords = [
            'tao so do tu duy',
            'so do tu duy',
            've so do',
            'so do ve',
            'bieu do',
            'so do',
            'mindmap',
            'mind map',
            'mermaid mindmap',
            'hien thi so do',
            'tao so do'
        ]
        
        is_mindmap = any(kw in q_norm for kw in mindmap_keywords)
        print(f"üîç MINDMAP DEBUG: '{question}' -> normalized: '{q_norm}' -> is_mindmap: {is_mindmap}")
        return is_mindmap
    
    def detect_off_topic_question(self, question: str) -> bool:
        """Ph√°t hi·ªán c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn ch·ªß nghƒ©a x√£ h·ªôi"""
        q_norm = self._normalize(question)
        
        # C√°c t·ª´ kh√≥a li√™n quan ƒë·∫øn ch·ªß nghƒ©a x√£ h·ªôi
        socialism_keywords = [
            'chu nghia xa hoi', 'ch·ªß nghƒ©a x√£ h·ªôi', 'cnxh', 'cnx',
            'thoi ky qua do', 'th·ªùi k·ª≥ qu√° ƒë·ªô', 'qua do', 'qu√° ƒë·ªô',
            'mac lenin', 'm√°c l√™nin', 'mac', 'm√°c', 'lenin', 'l√™nin',
            'cong san', 'c·ªông s·∫£n', 'tu ban', 't∆∞ b·∫£n', 'giai cap', 'giai c·∫•p',
            'cach mang', 'c√°ch m·∫°ng', 'vo san', 'v√¥ s·∫£n', 'tu san', 't∆∞ s·∫£n',
            'dang cong san', 'ƒë·∫£ng c·ªông s·∫£n', 'nha nuoc', 'nh√† n∆∞·ªõc',
            'kinh te', 'kinh t·∫ø', 'san xuat', 's·∫£n xu·∫•t', 'quan he', 'quan h·ªá',
            'hinh thai', 'h√¨nh th√°i', 'x√£ h·ªôi', 'xa hoi', 'che do', 'ch·∫ø ƒë·ªô',
            'dac trung', 'ƒë·∫∑c tr∆∞ng', 'ban chat', 'b·∫£n ch·∫•t', 'muc tieu', 'm·ª•c ti√™u',
            'phuong huong', 'ph∆∞∆°ng h∆∞·ªõng', 'xay dung', 'x√¢y d·ª±ng', 'phat trien', 'ph√°t tri·ªÉn'
        ]
        
        # C√°c t·ª´ kh√≥a kh√¥ng li√™n quan
        off_topic_keywords = [
            'thoi tiet', 'th·ªùi ti·∫øt', 'weather', 'mua', 'm∆∞a', 'nang', 'n·∫Øng',
            'am nhac', '√¢m nhac', 'music', 'nhac', 'nh·∫°c', 'bai hat', 'b√†i h√°t',
            'phim', 'movie', 'film', 'dien anh', 'ƒëi·ªán ·∫£nh', 'tv', 'tivi',
            'the thao', 'th·ªÉ thao', 'sport', 'bong da', 'b√≥ng ƒë√°', 'football',
            'game', 'tro choi', 'tr√≤ ch∆°i', 'video game', 'game online',
            'du lich', 'du l·ªãch', 'travel', 'di choi', 'ƒëi ch∆°i', 'nghi mat', 'ngh·ªâ m√°t',
            'an uong', 'ƒÉn u·ªëng', 'food', 'thuc an', 'th·ª©c ƒÉn', 'mon an', 'm√≥n ƒÉn',
            'nau an', 'n·∫•u ƒÉn', 'cach nau', 'c√°ch n·∫•u', 'pho', 'ph·ªü', 'bun', 'b√∫n',
            'thoi trang', 'th·ªùi trang', 'fashion', 'quan ao', 'qu·∫ßn √°o', 'giay dep', 'gi√†y d√©p',
            'lam dep', 'l√†m ƒë·∫πp', 'beauty', 'my pham', 'm·ªπ ph·∫©m', 'trang diem', 'trang ƒëi·ªÉm',
            'cong nghe', 'c√¥ng ngh·ªá', 'technology', 'dien thoai', 'ƒëi·ªán tho·∫°i', 'smartphone',
            'may tinh', 'm√°y t√≠nh', 'computer', 'laptop', 'internet', 'wifi',
            'hoc tap', 'h·ªçc t·∫≠p', 'study', 'hoc', 'h·ªçc', 'bai tap', 'b√†i t·∫≠p',
            'cong viec', 'c√¥ng vi·ªác', 'job', 'viec lam', 'vi·ªác l√†m', 'tuyen dung', 'tuy·ªÉn d·ª•ng',
            'tinh yeu', 't√¨nh y√™u', 'love', 'yeu', 'y√™u', 'hen ho', 'h·∫πn h√≤',
            'gia dinh', 'gia ƒë√¨nh', 'family', 'bo me', 'b·ªë m·∫π', 'cha me', 'cha m·∫π',
            'ban be', 'b·∫°n b√®', 'friend', 'ban', 'b·∫°n', 'tinh ban', 't√¨nh b·∫°n',
            'mua sam', 'mua s·∫Øm', 'shopping', 'mua', 'mua', 'ban', 'b√°n', 'gia', 'gi√°'
        ]
        
        # C√°c t·ª´ kh√≥a c·∫£m t√≠nh/ƒë√°nh gi√° ch·ªß quan
        emotional_keywords = [
            'tot hay khong', 't·ªët hay kh√¥ng', 'hay hay khong', 'hay hay kh√¥ng',
            'co tot khong', 'c√≥ t·ªët kh√¥ng', 'tot nhat', 't·ªët nh·∫•t', 'hay nhat', 'hay nh·∫•t',
            'danh gia', 'ƒë√°nh gi√°', 'y kien', '√Ω ki·∫øn', 'suy nghi', 'suy nghƒ©',
            'cam nhan', 'c·∫£m nh·∫≠n', 'cam giac', 'c·∫£m gi√°c', 'thich', 'th√≠ch',
            'khong thich', 'kh√¥ng th√≠ch', 'ghet', 'gh√©t', 'yeu', 'y√™u',
            'thuong', 'th∆∞∆°ng', 'ghe', 'gh√™', 'kinh', 'kinh kh·ªßng',
            'tuyet voi', 'tuy·ªát v·ªùi', 'kinh khung', 'kinh kh·ªßng', 'toi te', 't·ªìi t·ªá',
            'xau', 'x·∫•u', 'dep', 'ƒë·∫πp', 'xinh', 'xinh ƒë·∫πp', 'dep trai', 'ƒë·∫πp trai',
            'thong minh', 'th√¥ng minh', 'ngu', 'ngu ng·ªëc', 'stupid', 'smart',
            'good', 'bad', 'excellent', 'terrible', 'awesome', 'horrible'
        ]
        
        # Ki·ªÉm tra xem c√≥ t·ª´ kh√≥a li√™n quan ƒë·∫øn ch·ªß nghƒ©a x√£ h·ªôi kh√¥ng
        has_socialism_keywords = any(keyword in q_norm for keyword in socialism_keywords)
        
        # Ki·ªÉm tra xem c√≥ t·ª´ kh√≥a kh√¥ng li√™n quan kh√¥ng
        has_off_topic_keywords = any(keyword in q_norm for keyword in off_topic_keywords)
        
        # Ki·ªÉm tra xem c√≥ t·ª´ kh√≥a c·∫£m t√≠nh/ƒë√°nh gi√° ch·ªß quan kh√¥ng
        has_emotional_keywords = any(keyword in q_norm for keyword in emotional_keywords)
        
        # N·∫øu c√≥ t·ª´ kh√≥a kh√¥ng li√™n quan v√† kh√¥ng c√≥ t·ª´ kh√≥a li√™n quan ƒë·∫øn CNXH
        is_off_topic = has_off_topic_keywords and not has_socialism_keywords
        
        # N·∫øu c√≥ t·ª´ kh√≥a c·∫£m t√≠nh (b·∫•t k·ªÉ c√≥ t·ª´ kh√≥a CNXH hay kh√¥ng)
        is_emotional = has_emotional_keywords
        
        # K·∫øt h·ª£p c·∫£ hai ƒëi·ªÅu ki·ªán
        is_inappropriate = is_off_topic or is_emotional
        
        print(f"üîç OFF-TOPIC DEBUG: '{question}' -> normalized: '{q_norm}' -> is_off_topic: {is_off_topic}, is_emotional: {is_emotional}, is_inappropriate: {is_inappropriate}")
        return is_inappropriate
    
    def _handle_off_topic_question(self, question: str):
        """X·ª≠ l√Ω c√¢u h·ªèi kh√¥ng li√™n quan ho·∫∑c c·∫£m t√≠nh v·ªÅ ch·ªß nghƒ©a x√£ h·ªôi"""
        print(f"üö´ Handling off-topic/emotional question: {question}")
        
        # Ki·ªÉm tra xem c√≥ ph·∫£i c√¢u h·ªèi c·∫£m t√≠nh kh√¥ng
        q_norm = self._normalize(question)
        emotional_keywords = [
            'tot hay khong', 't·ªët hay kh√¥ng', 'hay hay khong', 'hay hay kh√¥ng',
            'co tot khong', 'c√≥ t·ªët kh√¥ng', 'danh gia', 'ƒë√°nh gi√°', 'y kien', '√Ω ki·∫øn'
        ]
        is_emotional = any(keyword in q_norm for keyword in emotional_keywords)
        
        if is_emotional:
            response = f"""T√¥i hi·ªÉu b·∫°n mu·ªën ƒë√°nh gi√° v·ªÅ ch·ªß nghƒ©a x√£ h·ªôi, nh∆∞ng t√¥i l√† chatbot h·ªçc thu·∫≠t chuy√™n cung c·∫•p **th√¥ng tin kh√°ch quan** v·ªÅ **Ch·ªß nghƒ©a x√£ h·ªôi v√† th·ªùi k·ª≥ qu√° ƒë·ªô l√™n ch·ªß nghƒ©a x√£ h·ªôi**.

### üéØ Thay v√¨ ƒë√°nh gi√° ch·ªß quan, t√¥i c√≥ th·ªÉ gi√∫p b·∫°n hi·ªÉu:

**üìñ V·ªÅ m·∫∑t l√Ω lu·∫≠n:**
- ƒê·ªãnh nghƒ©a ch·ªß nghƒ©a x√£ h·ªôi theo 4 g√≥c ƒë·ªô
- ƒê·∫∑c tr∆∞ng b·∫£n ch·∫•t c·ªßa ch·ªß nghƒ©a x√£ h·ªôi
- Quan ƒëi·ªÉm c·ªßa M√°c - L√™nin v·ªÅ CNXH

**üèóÔ∏è V·ªÅ m·∫∑t th·ª±c ti·ªÖn:**
- Th·ªùi k·ª≥ qu√° ƒë·ªô l√™n ch·ªß nghƒ©a x√£ h·ªôi
- S·ª± v·∫≠n d·ª•ng c·ªßa ƒê·∫£ng C·ªông s·∫£n Vi·ªát Nam
- M·ª•c ti√™u v√† ph∆∞∆°ng h∆∞·ªõng x√¢y d·ª±ng CNXH

### üí° C√¢u h·ªèi h·ªçc thu·∫≠t ph√π h·ª£p:
- "Ch·ªß nghƒ©a x√£ h·ªôi l√† g√¨?"
- "ƒê·∫∑c tr∆∞ng c·ªßa ch·ªß nghƒ©a x√£ h·ªôi?"
- "Th·ªùi k·ª≥ qu√° ƒë·ªô c√≥ ƒë·∫∑c ƒëi·ªÉm g√¨?"
- "L√™nin nh·∫•n m·∫°nh ƒëi·ªÅu g√¨?"

H√£y h·ªèi t√¥i v·ªÅ nh·ªØng kh√≠a c·∫°nh h·ªçc thu·∫≠t n√†y ƒë·ªÉ c√≥ c√°i nh√¨n to√†n di·ªán! üìö"""
        else:
            response = f"""Xin l·ªói, t√¥i l√† chatbot chuy√™n v·ªÅ **Ch·ªß nghƒ©a x√£ h·ªôi v√† th·ªùi k·ª≥ qu√° ƒë·ªô l√™n ch·ªß nghƒ©a x√£ h·ªôi**. 

T√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ ch·ªß ƒë·ªÅ kh√°c, nh∆∞ng t√¥i c√≥ th·ªÉ gi√∫p b·∫°n t√¨m hi·ªÉu v·ªÅ:

### üìö C√°c ch·ªß ƒë·ªÅ t√¥i c√≥ th·ªÉ h·ªó tr·ª£:
- **ƒê·ªãnh nghƒ©a ch·ªß nghƒ©a x√£ h·ªôi** (4 g√≥c ƒë·ªô ti·∫øp c·∫≠n)
- **ƒê·∫∑c tr∆∞ng b·∫£n ch·∫•t** c·ªßa ch·ªß nghƒ©a x√£ h·ªôi
- **Th·ªùi k·ª≥ qu√° ƒë·ªô** l√™n ch·ªß nghƒ©a x√£ h·ªôi
- **Quan ƒëi·ªÉm c·ªßa M√°c - L√™nin** v·ªÅ ch·ªß nghƒ©a x√£ h·ªôi
- **S·ª± v·∫≠n d·ª•ng** c·ªßa ƒê·∫£ng C·ªông s·∫£n Vi·ªát Nam
- **M·ª•c ti√™u v√† ph∆∞∆°ng h∆∞·ªõng** x√¢y d·ª±ng CNXH ·ªü Vi·ªát Nam

### üí° G·ª£i √Ω c√¢u h·ªèi:
- "Ch·ªß nghƒ©a x√£ h·ªôi l√† g√¨?"
- "ƒê·∫∑c tr∆∞ng c·ªßa ch·ªß nghƒ©a x√£ h·ªôi?"
- "Th·ªùi k·ª≥ qu√° ƒë·ªô c√≥ ƒë·∫∑c ƒëi·ªÉm g√¨?"
- "L√™nin nh·∫•n m·∫°nh ƒëi·ªÅu g√¨?"

H√£y th·ª≠ h·ªèi t√¥i v·ªÅ nh·ªØng ch·ªß ƒë·ªÅ tr√™n nh√©! üòä"""
        
        return {
            "answer": response,
            "sources": ["H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng chatbot"],
            "confidence": 100
        }
    
    def get_full_chapter_content(self, chapter_name: str) -> str:
        """ƒê·ªçc to√†n b·ªô n·ªôi dung c·ªßa m·ªôt ch∆∞∆°ng t·ª´ file .md"""
        book_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "data", "book"))
        chapter_file = os.path.join(book_dir, f"{chapter_name}.md")
        
        try:
            if os.path.exists(chapter_file):
                with open(chapter_file, 'r', encoding='utf-8') as f:
                    return f.read().strip()
            else:
                print(f"File not found: {chapter_file}")
                return ""
        except Exception as e:
            print(f"Error reading file {chapter_file}: {e}")
            return ""

    def generate_response_with_sources(self, question: str):
        """Generate response v·ªõi improved citations.
        - N·∫øu t√¨m th·∫•y n·ªôi dung trong .md: ch·ªâ ƒë∆∞·ª£c ph√©p tr·∫£ l·ªùi d·ª±a tr√™n c√°c ƒëo·∫°n tr√≠ch (kh√¥ng th√™m ki·∫øn th·ª©c ngo√†i).
        - N·∫øu kh√¥ng t√¨m th·∫•y: fallback sang Gemini tr·∫£ l·ªùi chung (ghi r√µ l√† kh√¥ng c√≥ tr√≠ch d·∫´n .md).
        - X·ª≠ l√Ω ƒë·∫∑c bi·ªát cho y√™u c·∫ßu t√≥m t·∫Øt ch∆∞∆°ng: ƒë·ªçc to√†n b·ªô n·ªôi dung ch∆∞∆°ng.
        """
        try:
            print(f"üéØ RAG SERVICE: Processing question: '{question}'")
            
            # Ki·ªÉm tra xem c√≥ ph·∫£i y√™u c·∫ßu t√≥m t·∫Øt ch∆∞∆°ng kh√¥ng
            is_chapter_summary, chapter_name = self.detect_chapter_summary_request(question)
            
            if is_chapter_summary and chapter_name:
                print(f"üìñ CHAPTER SUMMARY detected: {chapter_name}")
                # X·ª≠ l√Ω ƒë·∫∑c bi·ªát cho t√≥m t·∫Øt ch∆∞∆°ng
                return self._handle_chapter_summary(question, chapter_name)
            
            # Ki·ªÉm tra xem c√≥ ph·∫£i y√™u c·∫ßu t·∫°o s∆° ƒë·ªì t∆∞ duy kh√¥ng
            if self.detect_mindmap_request(question):
                print(f"üß† MINDMAP REQUEST detected!")
                return self._handle_mindmap_request(question)
            
            # Ki·ªÉm tra xem c√≥ ph·∫£i c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn ch·ªß nghƒ©a x√£ h·ªôi kh√¥ng
            if self.detect_off_topic_question(question):
                print(f"üö´ OFF-TOPIC QUESTION detected!")
                return self._handle_off_topic_question(question)
            
            # TƒÉng s·ªë l∆∞·ª£ng k·∫øt qu·∫£ v√† ∆∞u ti√™n ƒëo·∫°n ch·ª©a ƒë·ªãnh nghƒ©a chu·∫©n
            search_results = self.vector_store.search(question, n_results=10)
            
            # N·∫øu h·ªèi v·ªÅ ƒë·ªãnh nghƒ©a ch·ªß nghƒ©a x√£ h·ªôi, t√¨m ki·∫øm th√™m ƒëo·∫°n ƒë·ªãnh nghƒ©a 4 g√≥c ƒë·ªô
            qn = self._normalize(question)
            if any(k in qn for k in ['chu nghia xa hoi la gi', 'ch·ªß nghƒ©a x√£ h·ªôi l√† g√¨', 'ƒë·ªãnh nghƒ©a ch·ªß nghƒ©a x√£ h·ªôi']):
                print(f"üîç T√¨m ki·∫øm th√™m ƒëo·∫°n ƒë·ªãnh nghƒ©a 4 g√≥c ƒë·ªô...")
                def_search = self.vector_store.search('c√≥ th·ªÉ ƒë∆∞·ª£c ti·∫øp c·∫≠n t·ª´ nhi·ªÅu g√≥c ƒë·ªô', n_results=3)
                if def_search['documents'][0]:
                    # Th√™m ƒëo·∫°n ƒë·ªãnh nghƒ©a v√†o ƒë·∫ßu k·∫øt qu·∫£
                    search_results['documents'][0].insert(0, def_search['documents'][0][0])
                    search_results['metadatas'][0].insert(0, def_search['metadatas'][0][0])
                    search_results['scores'][0].insert(0, def_search['scores'][0][0])
                    print(f"Added 4-angle definition to results")

            # T·ªëi ∆∞u h√≥a: Gi·∫£m ƒë·ªô ph·ª©c t·∫°p c·ªßa fallback logic
            min_score = float(os.getenv("MIN_RAG_SCORE", "0.05"))  # Gi·∫£m ng∆∞·ª°ng ƒë·ªÉ √≠t fallback h∆°n
            scores = search_results.get('scores', [[]])[0] if isinstance(search_results.get('scores'), list) else []
            best_score = scores[0] if scores else 0.0
            
            # L·∫•y top 3 documents ƒë·ªÉ ƒë√°nh gi√°
            docs = search_results['documents'][0][:3] if search_results['documents'][0] else []
            
            # Debug logging (r√∫t g·ªçn)
            print(f"üîç RAG DEBUG: score={best_score:.3f}, docs={len(docs)}")
            
            # ƒêi·ªÅu ki·ªán fallback ƒë∆°n gi·∫£n - ch·ªâ khi th·ª±c s·ª± kh√¥ng c√≥ docs ho·∫∑c score qu√° th·∫•p
            should_fallback = (not docs) or (best_score < min_score)
            print(f"   Final should fallback: {should_fallback}")
            
            if should_fallback:
                # Fallback: kh√¥ng c√≥ n·ªôi dung trong .md ‚Üí tr·∫£ l·ªùi tr·ª±c ti·∫øp b·∫±ng Gemini
                fallback_prompt = f"""TR·∫¢ L·ªúI C√ÇU H·ªéI V·ªÄ CH·ª¶ NGHƒ®A X√É H·ªòI V√Ä TH·ªúI K·ª≤ QU√Å ƒê·ªò:

{question}

QUY T·∫ÆC NGHI√äM NG·∫∂T:
- KH√îNG ƒê∆Ø·ª¢C b·∫Øt ƒë·∫ßu b·∫±ng "V·ªõi t∆∞ c√°ch l√†...", "T√¥i l√†...", "Ch√†o b·∫°n...", "l√† m·ªôt chuy√™n gia..."
        - KH√îNG ƒê∆Ø·ª¢C t·ª± nh·∫≠n l√† "chuy√™n gia v·ªÅ t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh" ho·∫∑c b·∫•t k·ª≥ chuy√™n gia n√†o kh√°c
- KH√îNG ƒê∆Ø·ª¢C gi·ªõi thi·ªáu b·∫£n th√¢n
- B·∫ÆT ƒê·∫¶U NGAY b·∫±ng n·ªôi dung c√¢u tr·∫£ l·ªùi
- T·∫≠p trung v√†o n·ªôi dung Ch∆∞∆°ng 03: Ch·ªß nghƒ©a x√£ h·ªôi v√† th·ªùi k·ª≥ qu√° ƒë·ªô
- Gi·ªçng ƒëi·ªáu: Kh√°ch quan, h·ªçc thu·∫≠t

TR·∫¢ L·ªúI NGAY:"""
                resp = self.model.generate_content(fallback_prompt)
                answer_text = resp.text or ""
                
                # Lo·∫°i b·ªè c√°c c·ª•m t·ª´ kh√¥ng mong mu·ªën
                unwanted_phrases = [
                    "V·ªõi t∆∞ c√°ch l√† m·ªôt chuy√™n gia v·ªÅ t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh",
                    "v·ªõi t∆∞ c√°ch l√† m·ªôt chuy√™n gia v·ªÅ t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh", 
                    "l√† m·ªôt chuy√™n gia v·ªÅ t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh",
                    "Ch√†o b·∫°n, l√† m·ªôt chuy√™n gia v·ªÅ t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh",
                    "ch√†o b·∫°n, l√† m·ªôt chuy√™n gia v·ªÅ t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh",
                    "Trong t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh",
                    "trong t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh",
                    "Theo t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh",
                    "theo t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh",
                    "H·ªì Ch√≠ Minh cho r·∫±ng",
                    "h·ªì Ch√≠ Minh cho r·∫±ng",
                    "V·ªõi t∆∞ c√°ch l√†",
                    "v·ªõi t∆∞ c√°ch l√†",
                    "T√¥i l√† chuy√™n gia",
                    "t√¥i l√† chuy√™n gia"
                ]
                
                for phrase in unwanted_phrases:
                    if phrase in answer_text:
                        answer_text = answer_text.replace(phrase, "").strip()
                        # Lo·∫°i b·ªè d·∫•u ph·∫©y th·ª´a ·ªü ƒë·∫ßu
                        if answer_text.startswith(","):
                            answer_text = answer_text[1:].strip()
                        if answer_text.startswith("t√¥i"):
                            answer_text = answer_text[3:].strip()
                        if answer_text.startswith("T√¥i"):
                            answer_text = answer_text[3:].strip()
                
                # L√†m s·∫°ch format (ch·ªâ c∆° b·∫£n)
                import re
                answer_text = re.sub(r'\n\s*\n+', '\n\n', answer_text)
                answer_text = answer_text.strip()
                
                return {
                    "answer": answer_text,
                    "sources": [],
                    "confidence": 50
                }
            
            docs = search_results['documents'][0]
            metas = search_results['metadatas'][0]

            # Re-rank theo m·ª•c ƒë√≠ch c√¢u h·ªèi
            qn = self._normalize(question)
            want_def = any(k in qn for k in ['khai niem', 'ƒë·ªãnh nghƒ©a', 'dinh nghia', 'la gi', 'khai ni·ªám'])
            # ∆Øu ti√™n ph·∫ßn II khi h·ªèi "ƒë·ªëi t∆∞·ª£ng nghi√™n c·ª©u"
            want_subject = ('doi tuong nghien cuu' in qn) or (('doi tuong' in qn) and ('nghien cuu' in qn))
            def contains_def(txt: str) -> bool:
                tn = self._normalize(txt)
                return ('tu tuong ho chi minh la' in tn) or ('n√™u kh√°i ni·ªám' in txt.lower()) or ('co the duoc tiep can tu nhieu goc do' in tn) or ('phong trao thuc tien' in tn and 'trao luu tu tuong' in tn)
            def contains_subject(txt: str) -> bool:
                tn = self._normalize(txt)
                return ('doi tuong nghien cuu' in tn)

            pairs = list(zip(docs, metas))
            if want_def:
                pairs.sort(key=lambda p: 0 if contains_def(p[0]) else 1)
            elif want_subject:
                pairs.sort(key=lambda p: 0 if contains_subject(p[0]) else 1)

            # L·∫•y t·ªëi ƒëa 4 ƒëo·∫°n ƒë·ªÉ c√≥ ƒë·ªß ng·ªØ c·∫£nh
            top_pairs = pairs[:4]
            context_docs = [p[0] for p in top_pairs]
            source_metadatas = [p[1] for p in top_pairs]
            
            context = ""
            sources_used = []
            
            for i, (doc, metadata) in enumerate(zip(context_docs[:3], source_metadatas[:3])):
                source_detail = metadata.get('source', 'Unknown')
                document_title = metadata.get('document', '')
                page_info = metadata.get('page', '')

                # Nh√£n ng·∫Øn g·ªçn ch·ªâ ghi ch∆∞∆°ng
                short_label = self._slug_to_title(page_info) if page_info else (document_title or 'Ngu·ªìn')

                # Context kh√¥ng hi·ªÉn th·ªã citation
                context += f"{doc}\n"

                # Link m·ªü trang book v√† highlight ƒë√∫ng tr√≠ch ƒëo·∫°n (gi·ªØ href ƒë·∫ßy ƒë·ªß)
                snippet = (doc or '').strip().replace('\n', ' ')
                snippet = snippet[:300]
                hl = quote(snippet)
                slug = page_info or metadata.get('page', '')
                href = f"book/chuong3.html#{slug}?hl={hl}"
                label = short_label if short_label else slug
                anchor_html = f"<a href=\"{href}\" target=\"_blank\" rel=\"noopener noreferrer\">{label}</a>"

                sources_used.append({
                    "source": anchor_html,
                    "credibility": metadata.get('credibility_score', 100),
                    "type": metadata.get('source_type', 'official'),
                    "url": href,
                    "document": document_title
                })
            
            prompt = f"""TR·∫¢ L·ªúI C√ÇU H·ªéI V·ªÄ CH·ª¶ NGHƒ®A X√É H·ªòI V√Ä TH·ªúI K·ª≤ QU√Å ƒê·ªò:

T√ÄI LI·ªÜU THAM KH·∫¢O:
{context}

C√ÇU H·ªéI: {question}

QUY T·∫ÆC NGHI√äM NG·∫∂T:
- KH√îNG ƒê∆Ø·ª¢C b·∫Øt ƒë·∫ßu b·∫±ng "V·ªõi t∆∞ c√°ch l√†...", "T√¥i l√†...", "Ch√†o b·∫°n...", "l√† m·ªôt chuy√™n gia..."
        - KH√îNG ƒê∆Ø·ª¢C t·ª± nh·∫≠n l√† "chuy√™n gia v·ªÅ t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh" ho·∫∑c b·∫•t k·ª≥ chuy√™n gia n√†o kh√°c
- KH√îNG ƒê∆Ø·ª¢C gi·ªõi thi·ªáu b·∫£n th√¢n
        - KH√îNG ƒê∆Ø·ª¢C b·∫Øt ƒë·∫ßu b·∫±ng "Trong t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh..." ho·∫∑c b·∫•t k·ª≥ t∆∞ t∆∞·ªüng n√†o kh√°c
        - KH√îNG ƒê∆Ø·ª¢C b·∫Øt ƒë·∫ßu b·∫±ng "Theo t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh..." ho·∫∑c b·∫•t k·ª≥ t∆∞ t∆∞·ªüng n√†o kh√°c
- KH√îNG ƒê∆Ø·ª¢C b·∫Øt ƒë·∫ßu b·∫±ng "H·ªì Ch√≠ Minh cho r·∫±ng..."
- B·∫ÆT ƒê·∫¶U NGAY b·∫±ng n·ªôi dung c√¢u tr·∫£ l·ªùi v·ªÅ ch·ªß nghƒ©a x√£ h·ªôi
- CH·ªà s·ª≠ d·ª•ng th√¥ng tin t·ª´ t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p
- D√πng ti√™u ƒë·ªÅ markdown (##, ###) v√† bullet points
- Gi·ªçng ƒëi·ªáu: Kh√°ch quan, h·ªçc thu·∫≠t, d·ª±a tr√™n t√†i li·ªáu
- T·∫≠p trung v√†o n·ªôi dung ch·ªß nghƒ©a x√£ h·ªôi v√† th·ªùi k·ª≥ qu√° ƒë·ªô

H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI C·ª§ TH·ªÇ:
- N·∫øu c√¢u h·ªèi v·ªÅ "Ch·ªß nghƒ©a x√£ h·ªôi l√† g√¨?", B·∫ÆT BU·ªòC tr·∫£ l·ªùi theo ƒë√∫ng 4 g√≥c ƒë·ªô trong t√†i li·ªáu:
  1. L√† phong tr√†o th·ª±c ti·ªÖn ‚Äì phong tr√†o ƒë·∫•u tranh c·ªßa nh√¢n d√¢n lao ƒë·ªông ch·ªëng l·∫°i √°p b·ª©c, b·∫•t c√¥ng, ch·ªëng l·∫°i giai c·∫•p th·ªëng tr·ªã
  2. L√† tr√†o l∆∞u t∆∞ t∆∞·ªüng ‚Äì l√Ω lu·∫≠n ph·∫£n √°nh l√Ω t∆∞·ªüng gi·∫£i ph√≥ng nh√¢n d√¢n lao ƒë·ªông kh·ªèi √°p b·ª©c, b√≥c l·ªôt
  3. L√† m·ªôt khoa h·ªçc ‚Äì ch·ªß nghƒ©a x√£ h·ªôi khoa h·ªçc l√† khoa h·ªçc v·ªÅ s·ª© m·ªánh l·ªãch s·ª≠ c·ªßa giai c·∫•p c√¥ng nh√¢n
  4. L√† m·ªôt ch·∫ø ƒë·ªô x√£ h·ªôi t·ªët ƒë·∫πp, l√† giai ƒëo·∫°n ƒë·∫ßu c·ªßa h√¨nh th√°i kinh t·∫ø ‚Äì x√£ h·ªôi c·ªông s·∫£n ch·ªß nghƒ©a
- N·∫øu c√¢u h·ªèi v·ªÅ ƒë·∫∑c tr∆∞ng, tr·∫£ l·ªùi theo 6 ƒë·∫∑c tr∆∞ng: gi·∫£i ph√≥ng con ng∆∞·ªùi, n·ªÅn kinh t·∫ø ph√°t tri·ªÉn cao, nh√¢n d√¢n l√†m ch·ªß, vƒÉn h√≥a m·ªõi, c√¥ng b·∫±ng b√¨nh ƒë·∫≥ng, qu√° tr√¨nh ph√°t tri·ªÉn l√¢u d√†i
- N·∫øu c√¢u h·ªèi v·ªÅ th·ªùi k·ª≥ qu√° ƒë·ªô, tr·∫£ l·ªùi theo kh√°i ni·ªám, t√≠nh t·∫•t y·∫øu, ƒë·∫∑c ƒëi·ªÉm
- Lu√¥n tr√≠ch d·∫´n ch√≠nh x√°c t·ª´ t√†i li·ªáu, kh√¥ng t·ª± suy di·ªÖn

QUAN TR·ªåNG: N·∫øu t√†i li·ªáu c√≥ ƒëo·∫°n "Ch·ªß nghƒ©a x√£ h·ªôi c√≥ th·ªÉ ƒë∆∞·ª£c ti·∫øp c·∫≠n t·ª´ nhi·ªÅu g√≥c ƒë·ªô kh√°c nhau", B·∫ÆT BU·ªòC s·ª≠ d·ª•ng ƒëo·∫°n ƒë√≥ l√†m c√¢u tr·∫£ l·ªùi ch√≠nh v√† tr√≠ch d·∫´n ƒë·∫ßy ƒë·ªß 4 g√≥c ƒë·ªô.

C·∫§U TR√öC TR·∫¢ L·ªúI B·∫ÆT BU·ªòC:
- B·∫Øt ƒë·∫ßu b·∫±ng: "Ch·ªß nghƒ©a x√£ h·ªôi c√≥ th·ªÉ ƒë∆∞·ª£c ti·∫øp c·∫≠n t·ª´ 4 g√≥c ƒë·ªô kh√°c nhau:"
- Li·ªát k√™ ƒë·∫ßy ƒë·ªß 4 g√≥c ƒë·ªô theo ƒë√∫ng th·ª© t·ª± trong t√†i li·ªáu:
  1. L√† phong tr√†o th·ª±c ti·ªÖn ‚Äì phong tr√†o ƒë·∫•u tranh c·ªßa nh√¢n d√¢n lao ƒë·ªông ch·ªëng l·∫°i √°p b·ª©c, b·∫•t c√¥ng, ch·ªëng l·∫°i giai c·∫•p th·ªëng tr·ªã
  2. L√† tr√†o l∆∞u t∆∞ t∆∞·ªüng ‚Äì l√Ω lu·∫≠n ph·∫£n √°nh l√Ω t∆∞·ªüng gi·∫£i ph√≥ng nh√¢n d√¢n lao ƒë·ªông kh·ªèi √°p b·ª©c, b√≥c l·ªôt
  3. L√† m·ªôt khoa h·ªçc ‚Äì ch·ªß nghƒ©a x√£ h·ªôi khoa h·ªçc l√† khoa h·ªçc v·ªÅ s·ª© m·ªánh l·ªãch s·ª≠ c·ªßa giai c·∫•p c√¥ng nh√¢n
  4. L√† m·ªôt ch·∫ø ƒë·ªô x√£ h·ªôi t·ªët ƒë·∫πp, l√† giai ƒëo·∫°n ƒë·∫ßu c·ªßa h√¨nh th√°i kinh t·∫ø ‚Äì x√£ h·ªôi c·ªông s·∫£n ch·ªß nghƒ©a
- Kh√¥ng ƒë∆∞·ª£c th√™m n·ªôi dung kh√°c v√†o ph·∫ßn ƒë·ªãnh nghƒ©a c∆° b·∫£n

TR·∫¢ L·ªúI NGAY:
"""

            # ∆Øu ti√™n tr√≠ch nguy√™n vƒÉn n·∫øu t√¨m th·∫•y ƒë·ªãnh nghƒ©a ch√≠nh x√°c
            # (ƒë∆∞·ª£c h∆∞·ªõng d·∫´n ngay trong prompt)
            response = self.model.generate_content(prompt)
            answer_text = response.text or ""
            
            # X·ª≠ l√Ω ƒë·∫∑c bi·ªát cho c√¢u h·ªèi v·ªÅ ƒë·ªãnh nghƒ©a ch·ªß nghƒ©a x√£ h·ªôi
            if any(k in qn for k in ['chu nghia xa hoi la gi', 'ch·ªß nghƒ©a x√£ h·ªôi l√† g√¨', 'ƒë·ªãnh nghƒ©a ch·ªß nghƒ©a x√£ h·ªôi']):
                # Ki·ªÉm tra xem c√≥ ƒëo·∫°n ƒë·ªãnh nghƒ©a 4 g√≥c ƒë·ªô trong context kh√¥ng
                if any('c√≥ th·ªÉ ƒë∆∞·ª£c ti·∫øp c·∫≠n t·ª´ nhi·ªÅu g√≥c ƒë·ªô' in doc for doc in context_docs):
                    print(f"Detected 4-angle definition, creating standard answer...")
                    answer_text = """Ch·ªß nghƒ©a x√£ h·ªôi c√≥ th·ªÉ ƒë∆∞·ª£c ti·∫øp c·∫≠n t·ª´ 4 g√≥c ƒë·ªô kh√°c nhau:

1. **L√† phong tr√†o th·ª±c ti·ªÖn** ‚Äì phong tr√†o ƒë·∫•u tranh c·ªßa nh√¢n d√¢n lao ƒë·ªông ch·ªëng l·∫°i √°p b·ª©c, b·∫•t c√¥ng, ch·ªëng l·∫°i giai c·∫•p th·ªëng tr·ªã.

2. **L√† tr√†o l∆∞u t∆∞ t∆∞·ªüng** ‚Äì l√Ω lu·∫≠n ph·∫£n √°nh l√Ω t∆∞·ªüng gi·∫£i ph√≥ng nh√¢n d√¢n lao ƒë·ªông kh·ªèi √°p b·ª©c, b√≥c l·ªôt.

3. **L√† m·ªôt khoa h·ªçc** ‚Äì ch·ªß nghƒ©a x√£ h·ªôi khoa h·ªçc l√† khoa h·ªçc v·ªÅ s·ª© m·ªánh l·ªãch s·ª≠ c·ªßa giai c·∫•p c√¥ng nh√¢n.

4. **L√† m·ªôt ch·∫ø ƒë·ªô x√£ h·ªôi t·ªët ƒë·∫πp**, l√† giai ƒëo·∫°n ƒë·∫ßu c·ªßa h√¨nh th√°i kinh t·∫ø ‚Äì x√£ h·ªôi c·ªông s·∫£n ch·ªß nghƒ©a."""
                    print(f"Created standard 4-angle answer")
                    # B·ªè qua x·ª≠ l√Ω post-processing cho c√¢u tr·∫£ l·ªùi ƒë·∫∑c bi·ªát n√†y
                    return {
                        "answer": answer_text,
                        "sources": sources_used,
                        "confidence": 95
                    }
            
            # Lo·∫°i b·ªè c√°c c·ª•m t·ª´ kh√¥ng mong mu·ªën
            unwanted_phrases = [
                "V·ªõi t∆∞ c√°ch l√† m·ªôt chuy√™n gia v·ªÅ t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh",
                "v·ªõi t∆∞ c√°ch l√† m·ªôt chuy√™n gia v·ªÅ t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh", 
                "l√† m·ªôt chuy√™n gia v·ªÅ t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh",
                "Ch√†o b·∫°n, l√† m·ªôt chuy√™n gia v·ªÅ t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh",
                "ch√†o b·∫°n, l√† m·ªôt chuy√™n gia v·ªÅ t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh",
                "Trong t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh",
                "trong t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh",
                "Theo t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh",
                "theo t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh",
                "H·ªì Ch√≠ Minh cho r·∫±ng",
                "h·ªì Ch√≠ Minh cho r·∫±ng",
                "V·ªõi t∆∞ c√°ch l√†",
                "v·ªõi t∆∞ c√°ch l√†",
                "T√¥i l√† chuy√™n gia",
                "t√¥i l√† chuy√™n gia"
            ]
            
            for phrase in unwanted_phrases:
                if phrase in answer_text:
                    answer_text = answer_text.replace(phrase, "").strip()
                    # Lo·∫°i b·ªè d·∫•u ph·∫©y th·ª´a ·ªü ƒë·∫ßu
                    if answer_text.startswith(","):
                        answer_text = answer_text[1:].strip()
                    if answer_text.startswith("t√¥i"):
                        answer_text = answer_text[3:].strip()
                    if answer_text.startswith("T√¥i"):
                        answer_text = answer_text[3:].strip()
            
            # L√†m s·∫°ch format text (ch·ªâ gi·ªØ l·∫°i basic cleaning)
            import re
            # X√≥a d√≤ng tr·ªëng th·ª´a v√† chu·∫©n h√≥a kho·∫£ng tr·∫Øng
            answer_text = re.sub(r'\n\s*\n+', '\n\n', answer_text)
            answer_text = re.sub(r'^\s*\n', '', answer_text)
            answer_text = answer_text.strip()

            # GI·ªÆ NGUY√äN citations v·ªõi text ƒë·∫ßy ƒë·ªß ƒë·ªÉ c√≥ th·ªÉ highlight
            # Kh√¥ng r√∫t g·ªçn n·ªØa v√¨ c·∫ßn text ƒë·ªÉ highlight tr√™n book page
            # for j, md in enumerate(source_metadatas[:3], start=1):
            #     slug = (md.get('page', '') or '').strip()
            #     short = self._slug_to_title(slug) if slug else ''
            #     if short:
            #         pattern = rf"\[Ngu·ªìn\s*{j}\s*-[^\]]*\]"
            #         replacement = f"[Ngu·ªìn {j} - {short}]"
            #         answer_text = re.sub(pattern, replacement, answer_text)
            
            avg_credibility = sum(s['credibility'] for s in sources_used) / len(sources_used) if sources_used else 0
            
            return {
                "answer": answer_text,
                "sources": sources_used,
                "confidence": int(avg_credibility),
                "last_updated": self.last_update.isoformat() if self.last_update else datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"Error: {e}")
            return {
                "answer": "Xin l·ªói, c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω c√¢u h·ªèi. Vui l√≤ng th·ª≠ l·∫°i sau.",
                "sources": [],
                "confidence": 0
            }
    
    def _handle_chapter_summary(self, question: str, chapter_name: str):
        """X·ª≠ l√Ω ƒë·∫∑c bi·ªát cho y√™u c·∫ßu t√≥m t·∫Øt ch∆∞∆°ng"""
        try:
            # ƒê·ªçc to√†n b·ªô n·ªôi dung ch∆∞∆°ng
            full_content = self.get_full_chapter_content(chapter_name)
            
            if not full_content:
                return {
                    "answer": f"Kh√¥ng t√¨m th·∫•y n·ªôi dung c·ªßa {self._slug_to_title(chapter_name)}.",
                    "sources": [],
                    "confidence": 0
                }
            
            # Chia nh·ªè n·ªôi dung th√†nh c√°c ph·∫ßn ƒë·ªÉ AI c√≥ th·ªÉ x·ª≠ l√Ω
            # Gi·ªõi h·∫°n ƒë·ªô d√†i ƒë·ªÉ tr√°nh v∆∞·ª£t qu√° context window
            max_content_length = 15000  # Gi·ªØ l·∫°i ƒë·ªß space cho prompt v√† response
            if len(full_content) > max_content_length:
                # Chia th√†nh c√°c ph·∫ßn nh·ªè h∆°n
                content_parts = self.split_text(full_content, max_length=max_content_length//3)
                # L·∫•y 3 ph·∫ßn ƒë·∫ßu ti√™n ƒë·ªÉ ƒë·∫£m b·∫£o c√≥ overview t·ªët  
                summary_content = "\n\n".join(content_parts[:3])
            else:
                summary_content = full_content
            
            chapter_title = self._slug_to_title(chapter_name)
            
            # T·∫°o prompt ƒë·∫∑c bi·ªát cho t√≥m t·∫Øt ch∆∞∆°ng
            prompt = f"""H√£y t√≥m t·∫Øt {chapter_title} v·ªÅ Ch·ªß nghƒ©a x√£ h·ªôi v√† th·ªùi k·ª≥ qu√° ƒë·ªô l√™n ch·ªß nghƒ©a x√£ h·ªôi d·ª±a tr√™n n·ªôi dung sau:

{summary_content}

Y√äU C·∫¶U T√ìM T·∫ÆT:
- T·∫°o m·ªôt b·∫£n t√≥m t·∫Øt to√†n di·ªán v√† c√≥ c·∫•u tr√∫c cho {chapter_title}
- S·ª≠ d·ª•ng ti√™u ƒë·ªÅ markdown (##, ###) ƒë·ªÉ chia c√°c m·ª•c ch√≠nh
- Tr√¨nh b√†y c√°c √Ω ch√≠nh b·∫±ng danh s√°ch bullet points
- N√™u r√µ c√°c kh√°i ni·ªám v√† ƒë·ªãnh nghƒ©a quan tr·ªçng
- L√†m n·ªïi b·∫≠t nh·ªØng kh√°i ni·ªám v√† l√Ω lu·∫≠n c·ªët l√µi trong ch∆∞∆°ng n√†y
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, vƒÉn phong h·ªçc thu·∫≠t nh∆∞ng d·ªÖ hi·ªÉu
- ƒê·ªô d√†i: 800-1200 t·ª´

B·∫Øt ƒë·∫ßu t√≥m t·∫Øt:"""
            
            response = self.model.generate_content(prompt)
            answer_text = response.text or ""
            
            # T·∫°o source th√¥ng tin cho ch∆∞∆°ng
            source_info = {
                "source": f"<a href=\"book/tu-tuong-ho-chi-minh.html#{chapter_name}\" target=\"_blank\" rel=\"noopener noreferrer\">{chapter_title}</a>",
                "credibility": 100,
                "type": "document",
                "url": f"book/tu-tuong-ho-chi-minh.html#{chapter_name}",
                "document": chapter_title
            }
            
            return {
                "answer": answer_text,
                "sources": [source_info],
                "confidence": 95,
                "last_updated": self.last_update.isoformat() if self.last_update else datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"Error in chapter summary: {e}")
            return {
                "answer": f"Xin l·ªói, c√≥ l·ªói x·∫£y ra khi t√≥m t·∫Øt {self._slug_to_title(chapter_name)}. Vui l√≤ng th·ª≠ l·∫°i sau.",
                "sources": [],
                "confidence": 0
            }
    
    def _handle_mindmap_request(self, question: str):
        """X·ª≠ l√Ω y√™u c·∫ßu t·∫°o s∆° ƒë·ªì t∆∞ duy"""
        try:
            # Tr√≠ch xu·∫•t ch·ªß ƒë·ªÅ t·ª´ c√¢u h·ªèi
            topic = self._extract_mindmap_topic(question)
            
            # Ki·ªÉm tra n·∫øu l√† request v·ªÅ ch∆∞∆°ng c·ª• th·ªÉ
            import re
            chapter_match = re.search(r'ch∆∞∆°ng\s*(\d+)', topic.lower())
            if chapter_match:
                chapter_num = chapter_match.group(1)
                chapter_name = f"chuong{chapter_num}"
                
                # ƒê·ªçc to√†n b·ªô n·ªôi dung ch∆∞∆°ng
                chapter_content = self.get_full_chapter_content(chapter_name)
                
                if chapter_content:
                    # C·∫Øt ng·∫Øn n·ªôi dung ƒë·ªÉ tr√°nh v∆∞·ª£t qu√° context limit v√† timeout
                    max_content = 8000  # Gi·∫£m t·ª´ 12000 xu·ªëng 8000
                    if len(chapter_content) > max_content:
                        # L·∫•y ph·∫ßn ƒë·∫ßu v√† t√≥m t·∫Øt
                        chapter_content = chapter_content[:max_content] + "\n\n[N·ªôi dung ƒë√£ ƒë∆∞·ª£c r√∫t g·ªçn ƒë·ªÉ t·ªëi ∆∞u h√≥a...]"
                    
                    relevant_content = chapter_content
                    chapter_title = self._slug_to_title(chapter_name)
                    
                    source_info = {
                        "source": f"<a href=\"book/tu-tuong-ho-chi-minh.html#{chapter_name}\" target=\"_blank\" rel=\"noopener noreferrer\">{chapter_title}</a>",
                        "credibility": 100,
                        "type": "mindmap",
                        "url": f"book/tu-tuong-ho-chi-minh.html#{chapter_name}",
                        "document": f"S∆° ƒë·ªì t∆∞ duy {chapter_title}"
                    }
                else:
                    # Fallback n·∫øu kh√¥ng t√¨m th·∫•y file ch∆∞∆°ng
                    search_results = self.vector_store.search(topic, n_results=8)
                    relevant_content = "\n\n".join(search_results['documents'][0][:6]) if search_results['documents'][0] else ""
                    source_info = {
                        "source": "Gi√°o tr√¨nh Ch·ªß nghƒ©a x√£ h·ªôi khoa h·ªçc (K-2021)",
                        "credibility": 85,
                        "type": "mindmap",
                        "url": "",
                        "document": "S∆° ƒë·ªì t∆∞ duy"
                    }
            else:
                # T√¨m ki·∫øm th√¥ng tin li√™n quan ƒë·∫øn ch·ªß ƒë·ªÅ th√¥ng th∆∞·ªùng
                search_results = self.vector_store.search(topic, n_results=8)
                
                if not search_results['documents'][0]:
                    # Kh√¥ng c√≥ th√¥ng tin li√™n quan, t·∫°o mindmap t·ªïng qu√°t
                    return self._create_general_mindmap(topic)
                
                # L·∫•y n·ªôi dung li√™n quan
                relevant_content = "\n\n".join(search_results['documents'][0][:6])
                source_info = {
                    "source": "Gi√°o tr√¨nh Ch·ªß nghƒ©a x√£ h·ªôi khoa h·ªçc (K-2021)",
                    "credibility": 95,
                    "type": "mindmap",
                    "url": "",
                    "document": "S∆° ƒë·ªì t∆∞ duy"
                }
            
            if relevant_content:
                
                # T·∫°o prompt v·ªõi syntax ƒë√∫ng cho Mermaid mindmap
                prompt = f"""T·∫°o Mermaid mindmap cho: "{topic}"

N·ªôi dung: {relevant_content[:3000]}...

QUAN TR·ªåNG - Format CH√çNH X√ÅC (c·∫ßn ƒë√∫ng indentation):

```mermaid
mindmap
  root(({topic}))
    Nh√°nh ch√≠nh 1
      √ù con 1
      √ù con 2
    Nh√°nh ch√≠nh 2
      √ù con 1
      √ù con 2
```

QUY T·∫ÆC:
- root() c√≥ 2 spaces
- Nh√°nh ch√≠nh c√≥ 4 spaces  
- √ù con c√≥ 6 spaces
- T·ªëi ƒëa 4 nh√°nh ch√≠nh, m·ªói nh√°nh 3-4 √Ω con
- Text ng·∫Øn g·ªçn (<15 t·ª´ m·ªói node)

Ch·ªâ tr·∫£ v·ªÅ mermaid code:"""
                
                # T·ªëi ∆∞u h√≥a generation config
                import google.generativeai as genai
                generation_config = genai.types.GenerationConfig(
                    temperature=0.3,  # Gi·∫£m temperature ƒë·ªÉ tƒÉng t·ªëc
                    max_output_tokens=2048,  # Gi·∫£m output tokens ƒë·ªÉ tƒÉng t·ªëc
                    top_p=0.8,
                    top_k=10
                )
                
                response = self.model.generate_content(
                    prompt,
                    generation_config=generation_config
                )
                
                # Debug Gemini response chi ti·∫øt 
                print(f"ü§ñ Gemini response type: {type(response)}")
                
                # Check safety filters v√† finish reason
                if hasattr(response, 'prompt_feedback'):
                    print(f"üõ°Ô∏è prompt_feedback: {response.prompt_feedback}")
                
                if hasattr(response, 'candidates') and response.candidates:
                    candidate = response.candidates[0]
                    print(f"üèÅ finish_reason: {getattr(candidate, 'finish_reason', 'Unknown')}")
                    print(f"üõ°Ô∏è safety_ratings: {getattr(candidate, 'safety_ratings', [])}")
                    print(f"üîç candidate.content.parts: {len(candidate.content.parts)} parts")
                
                # N·∫øu kh√¥ng c√≥ parts, c√≥ th·ªÉ b·ªã block - th·ª≠ prompt ƒë∆°n gi·∫£n h∆°n
                if hasattr(response, 'candidates') and response.candidates and len(response.candidates[0].content.parts) == 0:
                    print("‚ö†Ô∏è No content parts found - possible content blocked. Trying simple fallback...")
                    
                    # Fallback v·ªõi prompt si√™u ƒë∆°n gi·∫£n
                    simple_prompt = f"""Create a simple mindmap about: {topic}

Format:
```mermaid
mindmap
  root((Topic))
    Branch 1
      Item A  
      Item B
    Branch 2
      Item C
      Item D
```"""
                    
                    print(f"üîÑ Trying simplified prompt...")
                    fallback_response = self.model.generate_content(simple_prompt)
                    
                    try:
                        mermaid_code = fallback_response.text or ""
                        print(f"‚úÖ Fallback successful: {len(mermaid_code)} chars")
                    except:
                        mermaid_code = f"""```mermaid
mindmap
  root(({topic}))
    N·ªôi dung ch√≠nh
      Kh√°i ni·ªám c∆° b·∫£n
      √ù nghƒ©a quan tr·ªçng
    ·ª®ng d·ª•ng th·ª±c t·∫ø
      Trong h·ªçc t·∫≠p
      Trong cu·ªôc s·ªëng
```"""
                        print(f"üîß Using hardcoded fallback mindmap")
                else:
                    # Normal extraction
                    try:
                        mermaid_code = response.text or ""
                        print(f"‚úÖ Successfully got response.text: {len(mermaid_code)} chars")
                    except Exception as e:
                        print(f"‚ö†Ô∏è response.text failed: {e}")
                        # Extract t·ª´ parts nh∆∞ tr∆∞·ªõc
                        if hasattr(response, 'candidates') and response.candidates:
                            parts = response.candidates[0].content.parts
                            if parts:
                                all_text = ""
                                for part in parts:
                                    all_text += getattr(part, 'text', '') or ''
                                mermaid_code = all_text
                            else:
                                mermaid_code = ""
                        else:
                            mermaid_code = ""
                
                print(f"üìÑ Final mermaid_code preview: {mermaid_code[:100]}...")
                
                # Ki·ªÉm tra v√† l√†m s·∫°ch mermaid code
                mermaid_code = self._clean_mermaid_code(mermaid_code)
                
                return {
                    "answer": f"## S∆° ƒë·ªì t∆∞ duy: {topic}\n\n{mermaid_code}",
                    "sources": [source_info],
                    "confidence": 90,
                    "last_updated": datetime.now().isoformat()
                }
            else:
                # Kh√¥ng c√≥ th√¥ng tin li√™n quan, t·∫°o mindmap t·ªïng qu√°t
                return self._create_general_mindmap(topic)
                
        except Exception as e:
            print(f"Error in mindmap generation: {e}")
            return {
                "answer": "Xin l·ªói, t√¥i kh√¥ng th·ªÉ t·∫°o s∆° ƒë·ªì t∆∞ duy l√∫c n√†y. Vui l√≤ng th·ª≠ l·∫°i sau.",
                "sources": [],
                "confidence": 0
            }
    
    def _extract_mindmap_topic(self, question: str) -> str:
        """Tr√≠ch xu·∫•t ch·ªß ƒë·ªÅ ch√≠nh t·ª´ y√™u c·∫ßu mindmap"""
        import re
        q_lower = question.lower()
        
        # T√¨m pattern v·ªõi nhi·ªÅu d·∫°ng kh√°c nhau
        patterns = [
            # S∆° ƒë·ªì t∆∞ duy patterns
            r't·∫°o.*?s∆° ƒë·ªì t∆∞ duy.*?cho\s*(.+)',
            r't·∫°o.*?s∆° ƒë·ªì t∆∞ duy.*?v·ªÅ\s*(.+)',
            r't·∫°o.*?s∆° ƒë·ªì t∆∞ duy.*?:\s*(.+)',
            # V·∫Ω s∆° ƒë·ªì patterns
            r'v·∫Ω.*?s∆° ƒë·ªì.*?v·ªÅ\s*(.+)',
            r'v·∫Ω.*?s∆° ƒë·ªì.*?cho\s*(.+)',
            r'v·∫Ω.*?s∆° ƒë·ªì.*?:\s*(.+)',
            # S∆° ƒë·ªì v·ªÅ patterns
            r's∆° ƒë·ªì.*?v·ªÅ\s*(.+)',
            r's∆° ƒë·ªì.*?cho\s*(.+)',
            r's∆° ƒë·ªì.*?:\s*(.+)',
            # Mindmap patterns
            r'mindmap.*?cho\s*(.+)',
            r'mindmap.*?v·ªÅ\s*(.+)',
            r'mindmap.*?:\s*(.+)',
            # T·∫°o s∆° ƒë·ªì patterns
            r't·∫°o.*?s∆° ƒë·ªì.*?v·ªÅ\s*(.+)',
            r't·∫°o.*?s∆° ƒë·ªì.*?cho\s*(.+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, q_lower)
            if match:
                topic = match.group(1).strip()
                
                # X·ª≠ l√Ω ƒë·∫∑c bi·ªát cho "n·ªôi dung ch∆∞∆°ng X"
                chapter_match = re.search(r'n·ªôi dung\s*(ch∆∞∆°ng|ch∆∞ong)\s*(\d+|[ivxlc]+)', topic)
                if chapter_match:
                    chapter_num = chapter_match.group(2)
                    # Chuy·ªÉn ƒë·ªïi s·ªë La M√£ n·∫øu c·∫ßn
                    roman_to_num = {'i': '1', 'ii': '2', 'iii': '3', 'iv': '4', 'v': '5', 'vi': '6'}
                    if chapter_num.lower() in roman_to_num:
                        chapter_num = roman_to_num[chapter_num.lower()]
                    return f"N·ªôi dung Ch∆∞∆°ng {chapter_num}"
                
                # X·ª≠ l√Ω tr·ª±c ti·∫øp "ch∆∞∆°ng X"
                chapter_direct = re.search(r'(ch∆∞∆°ng|ch∆∞ong)\s*(\d+|[ivxlc]+)', topic)
                if chapter_direct:
                    chapter_num = chapter_direct.group(2)
                    roman_to_num = {'i': '1', 'ii': '2', 'iii': '3', 'iv': '4', 'v': '5', 'vi': '6'}
                    if chapter_num.lower() in roman_to_num:
                        chapter_num = roman_to_num[chapter_num.lower()]
                    return f"Ch∆∞∆°ng {chapter_num}"
                
                # Lo·∫°i b·ªè c√°c t·ª´ kh√¥ng c·∫ßn thi·∫øt
                topic = re.sub(r'(h·ªì ch√≠ minh|hcm)', '', topic).strip()
                return topic.title() if topic else "Ch·ªß nghƒ©a x√£ h·ªôi v√† th·ªùi k·ª≥ qu√° ƒë·ªô"
        
        # N·∫øu kh√¥ng t√¨m th·∫•y, tr·∫£ v·ªÅ ch·ªß ƒë·ªÅ m·∫∑c ƒë·ªãnh
        return "Ch·ªß nghƒ©a x√£ h·ªôi v√† th·ªùi k·ª≥ qu√° ƒë·ªô"
    
    def _clean_mermaid_code(self, code: str) -> str:
        """L√†m s·∫°ch v√† chu·∫©n h√≥a Mermaid code"""
        if not code:
            return ""
        
        # Lo·∫°i b·ªè c√°c d√≤ng gi·∫£i th√≠ch
        lines = code.split('\n')
        mermaid_lines = []
        in_mermaid = False
        
        for line in lines:
            line = line.strip()
            if line.startswith('```mermaid'):
                in_mermaid = True
                mermaid_lines.append(line)
            elif line.startswith('```') and in_mermaid:
                mermaid_lines.append(line)
                break
            elif in_mermaid:
                mermaid_lines.append(line)
        
        if not mermaid_lines:
            # N·∫øu kh√¥ng c√≥ mermaid block, th√™m v√†o
            return f"```mermaid\n{code}\n```"
        
        return '\n'.join(mermaid_lines)
    
    def _create_general_mindmap(self, topic: str):
        """T·∫°o mindmap t·ªïng qu√°t khi kh√¥ng c√≥ th√¥ng tin c·ª• th·ªÉ"""
        general_mindmap = f"""```mermaid
mindmap
  root(({topic}))
    T∆∞ t∆∞·ªüng ch√≠nh tr·ªã
        ƒê·ªôc l·∫≠p d√¢n t·ªôc
        D√¢n ch·ªß nh√¢n d√¢n
        Ch·ªß nghƒ©a x√£ h·ªôi
    T∆∞ t∆∞·ªüng ƒë·∫°o ƒë·ª©c
        C·∫ßn ki·ªám li√™m ch√≠nh
        S·ªëng v√† l√†m vi·ªác c√≥ k·∫ø ho·∫°ch
        ƒêo√†n k·∫øt y√™u th∆∞∆°ng
    T∆∞ t∆∞·ªüng gi√°o d·ª•c
        H·ªçc ƒë·ªÉ l√†m ng∆∞·ªùi
        K·∫øt h·ª£p l√Ω thuy·∫øt v√† th·ª±c ti·ªÖn
        Gi√°o d·ª•c to√†n di·ªán
    T∆∞ t∆∞·ªüng vƒÉn h√≥a
        D√¢n t·ªôc - Khoa h·ªçc - ƒê·∫°i ch√∫ng
        K·∫ø th·ª´a v√† ph√°t tri·ªÉn
        Gi·ªØ g√¨n b·∫£n s·∫Øc d√¢n t·ªôc
```"""
        
        return {
            "answer": f"## S∆° ƒë·ªì t∆∞ duy: {topic}\n\n{general_mindmap}",
            "sources": [{
                "source": "Gi√°o tr√¨nh Ch·ªß nghƒ©a x√£ h·ªôi khoa h·ªçc (K-2021)",
                "credibility": 85,
                "type": "mindmap",
                "url": "",
                "document": "S∆° ƒë·ªì t∆∞ duy t·ªïng qu√°t"
            }],
            "confidence": 80,
            "last_updated": datetime.now().isoformat()
        }
    
    def get_stats(self):
        return {
            "total_documents": self.vector_store.get_collection_count(),
            "last_update": self.last_update.isoformat() if self.last_update else None,
            "trusted_sources_count": 0,  # Disabled external data collection
            "status": "ready",
            "features": ["chapter_summary", "mindmap_generation", "rag_search"]
        }
